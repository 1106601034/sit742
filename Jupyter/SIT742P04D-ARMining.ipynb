{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIT742: Modern Data Science \n",
    "**(Module 04: Exploratory Data Analysis)**\n",
    "\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Session 4D - Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Algorithm for Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Learning is a prominent area of Data Mining.\n",
    "From a given transcation dataset, the goal is to find a pattern among different items in transactions.\n",
    "This means investigating, to what extent does purchasing a item A leads to purchasing item B. On one hand,\n",
    "it helps the customer finding the items associated with eahc in a store. On the other hand, salesperson can\n",
    "use this information and place associated items together to escalate sales. <br/>\n",
    "One way for Association Rule Learning is Apriori Algorithm.\n",
    "Apriori algorithm uses 2 concepts from Association Rule Mining: <br/>\n",
    "    -Support <br/>\n",
    "    -Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will briefly discuss the two below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a transaction dataset T, Support is the proportion of the transactions that contain both the item A and B. <br/>\n",
    "Support (AB) = P(A $ \\cup $ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a transaction dataset T, Confidence is the percentage of transactions in T, containing A and that also contain B. In other words, this is the probability of B in T given that A is already in that transaction. <br/> Confidence (A $ \\rightarrow $ B) = P(B | A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apiori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apiori Algorithm is a Association Rule learning Algorithm and is used to find frequent itemsets and relevant association rules. By frequent itemsets we should think of items who are frequently associated with each other within the whole dataset. In order for a frequent itemset to be valid the subsets of the itemset needs to be frequent by themselves, this is called anti-monotonicity or downward-closure poperty. The anti-monotonicity guarantees a efficient search, this is necessary because the algorithm goes through all combinations adding one at a time. The process of adding one at a time is called buttom up approach.\n",
    "\n",
    "### `apyori` Package\n",
    "\n",
    "In this part, we will use the `apyori` package. \n",
    "\n",
    "Here we assume a simple transaction set, and then the calling of `apriori` is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "transactions = [\n",
    "    ['beer', 'nuts'],\n",
    "    ['beer', 'cheese'],\n",
    "]\n",
    "\n",
    "rules = apriori(transactions)\n",
    "\n",
    "# rules = apriori(transactions, min_support = 0.5, min_confidence = 0.7, min_length = 2)\n",
    "\n",
    "\n",
    "results = list(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    print(\"##############################################################################\")\n",
    "    print(i)\n",
    "    print(results[i])\n",
    "    print(results[i].items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions from File\n",
    "\n",
    "If the transaction set is stored on an external file, we might need to use some code to load it into the proper format:\n",
    "\n",
    "Consider a file with the following content:\n",
    "\n",
    "<img src=\"https://github.com/tulip-lab/sit742/blob/master/Jupyter/image/csv.JPG?raw=true\">\n",
    "When importing our dataset `Market_Basket_Optimization`, we should add the `header` argument with the value `None`, without this Python places the first row of the dataset as the titles of the columns and we do not want this. If we take a look at the dataframe now we can see that our product names are now located in the first row. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/Market_Basket_Optimisation.csv'\n",
    "DataSet = wget.download(link_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data Preprocessing\n",
    "dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is that we will preprocess our data set by placing it in Lists of Lists, we do this using two for loops. The first for loop goes through all the rows (transactions) of the dataset and the second for loop goes through all the products and adds these to a list before appending them to the transactions list. We can see that our dataset is now of the type list and that it's value is indeed a list of products. \n",
    "\n",
    "<img src=\"https://github.com/tulip-lab/sit742/blob/master/Jupyter/image/head.JPG?raw=true\">\n",
    "\n",
    "\n",
    "We will not go into details of this code because there are various ways to get to the same result with Python, just note that the second for loop is actually a inner for loop, nested in the append method. \n",
    "\n",
    "Also note that the `dataset.values[i,j]` is converted to a string value when appended to the transaction list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "for i in range(0, 7501):\n",
    "    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])\n",
    "print(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our apriori algorithm on the dataset, let's import the apyori.py library which contains the classes we will be using. We will declare a variable called rules since the apriori algorithm takes the transactions list as input and outputs association rules. If we inspect our Apiori function we see it accepts one main argument and that is an iterable object our transactions list. We can also specify the support, confidence and lift thresholds. A new argument which we have not discussed yet is min_lenght, it specify's the minimum length of our rules, we will set this to 2 to prevent rules with only one item.\n",
    "\n",
    "When deciding on the minimum support we should calculate this in a way that it makes sense according to the problem at hand. We want to optimize the sales for a supermarket, the dataset contains all the transactions made in one week. If we want to have products who are minimally bought 3 times a day our minimum support will be 0.0028. (3*7/7500)\n",
    "\n",
    "The minimum confidence should prevent that rules are made based on the wrong reasons, there are products who are bought frequently together not because they're strongly associated but because they are frequently bought products independent of each other. We want our rules to be relevant, choosing 0.8 is to high and will give us no results on this dataset, 0.4 will give us irrelevant rules as described earlier, the winner is 0.2 because this confidence minimum gave us relevant rules that were proven true enough times but not too frequent to be irrelevant to our goal. This minimum is found by running the Apriori algorithm different times and checking if the results makes sense. \n",
    "\n",
    "The lift measurement is a good indication of the relevance of our rules, we will set our minimum lift to 3 because we can be sure that these rules will be significant. Remember that these arguments depends on your dataset or problem. The Apriori is an experimental algorithm, we need to try different threshold and see if the produced rules have the wanted effect in the real world if not than we have to tune our thresholds some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Apriori on the dataset\n",
    "from apyori import apriori\n",
    "rules = apriori(transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2)\n",
    "\n",
    "# Visualising the results\n",
    "results = list(rules)\n",
    "\n",
    "myResults = [list(x) for x in results] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    print(\"##############################################################################\")\n",
    "    print(i)\n",
    "    print(results[i])\n",
    "    print(results[i].items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "As you can see the Apriori algorithm is way more efficient than the naive approach because it generates less candidates to test. However, multiple passes through the database are necessary which is not optimal. And particularly the **second pass is computationally intensive** because the set of candidates to test is the longest. This means that for each transaction, each candidate in the set need to be tested.... The second algorithm we will cover tries to address this problem specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-Tree\n",
    "This algorithm tries to store the frequent itemsets more efficiently. The retrieval of those is also reimagined. I made a python implementation of this interesting algorithm. You can check it here: __[Python implementation](https://github.com/DamDRAIME/DM---Frequent-Pattern-Trees)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic workflow of the FP-growth algorithm is divided into two steps ：**1. Build the FP tree**，The original data set needs to be scanned twice, the first pass counts the number of occurrences of all element items, and the second pass only considers those frequent elements；**2. Mining frequent itemsets.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compared with Apriori:\n",
    "The FP-growth algorithm only needs to scan the database twice, while the Apriori algorithm scans the data set for each potential frequent item set to determine whether a given pattern is frequent. Therefore, the FP-growth algorithm is faster than the Apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data structure of the FP tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP-tree class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class treeNode:\n",
    "    def __init__(self, nameValue, numOccur, parentNode):\n",
    "        self.name = nameValue#node name\n",
    "        self.count = numOccur#count of nodes\n",
    "        self.nodeLink = None #link similar items\n",
    "        self.parent = parentNode#refer to parent of the node\n",
    "        self.children = {}#children of the node\n",
    "\n",
    "    def inc(self,numOccur):\n",
    "        self.count += numOccur#increments the count\n",
    "\n",
    "    def disp(self, ind = 1):\n",
    "        print('  ' * ind, self.name, ' ', self.count)\n",
    "        for child in self.children.values():#for debugging\n",
    "            #ind += 1\n",
    "            child.disp(ind + 1)\n",
    "            #print(\"count:%d\" % (ind+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootNode = treeNode('pyramid',9,None)\n",
    "rootNode.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootNode.children['eye'] = treeNode('eye',13,None)\n",
    "rootNode.children['phoenix'] = treeNode('phoenix',3,None)\n",
    "rootNode.children['phoe'] = treeNode('phoe',23,None)\n",
    "rootNode.disp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build FP tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/tulip-lab/sit742/blob/master/Jupyter/image/buildFPtree.JPG?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head pointer schematic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/tulip-lab/sit742/blob/master/Jupyter/image/table12.JPG?raw=true\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/tulip-lab/sit742/blob/master/Jupyter/image/2.JPG?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two steps of constructing FP tree using table12-2 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP-tree creation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FP-tree\n",
    "def createTree(dataSet, minSup = 1):\n",
    "\theaderTable = {}\n",
    "\tfor trans in dataSet:\n",
    "\t\tfor item in trans:\n",
    "\t\t\theaderTable[item] = headerTable.get(item, 0) + dataSet[trans]\n",
    "\n",
    "\tfor k in list(headerTable.keys()):#remove items not meeting min support\n",
    "\t\tif headerTable[k] < minSup:\n",
    "\t\t\tdel(headerTable[k])\n",
    "\n",
    "\tfreqItemSet = set(headerTable.keys())\n",
    "\tif len(freqItemSet) == 0:#if no items meet min support,exit\n",
    "\t\treturn None,None\n",
    "\tfor k in headerTable:\n",
    "\t\theaderTable[k] = [headerTable[k], None]\n",
    "\tretTree = treeNode('Null Set', 1, None)\n",
    "\tfor tranSet, count in dataSet.items():\n",
    "\t\tlocalD = {}\n",
    "\t\tfor item in tranSet:#sort transactions by global frequency\n",
    "\t\t\tif item in freqItemSet:\n",
    "\t\t\t\tlocalD[item] = headerTable[item][0]\n",
    "\t\tif len(localD) > 0:\n",
    "\t\t\torderedItems = [v[0] for v in sorted(localD.items(),key = lambda p:p[1], reverse = True)]\n",
    "\t\t\tupdateTree(orderedItems, retTree, headerTable,count)#populate tree with ordered freq itemset\n",
    "\treturn retTree, headerTable\n",
    "\n",
    "def updateTree(items, inTree, headerTable, count):\n",
    "\tif items[0] in inTree.children:#exist\n",
    "\t\tinTree.children[items[0]].inc(count)\n",
    "\telse:#doesn't exist, create a new treeNode and adds it as a child\n",
    "\t\tinTree.children[items[0]] = treeNode(items[0], count, inTree)\n",
    "\t\tif headerTable[items[0]][1] == None:\n",
    "\t\t\theaderTable[items[0]][1] = inTree.children[items[0]]\n",
    "\t\telse:\n",
    "\t\t\tupdateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
    "\tif len(items) > 1:#recursively call updateTree on remaining items\n",
    "\t\tupdateTree(items[1::],inTree.children[items[0]],headerTable,count)\n",
    "\n",
    "def updateHeader(nodeToTest, targetNode):#nodeLink, linked list\n",
    "\twhile(nodeToTest.nodeLink != None):\n",
    "\t\tnodeToTest = nodeToTest.nodeLink\n",
    "\tnodeToTest.nodeLink = targetNode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Dataset and Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSimpDat():\n",
    "\tsimpDat = [['r','z','h','j','p'],\n",
    "\t\t\t\t['z','y','x','w','v','u','t','s'],\n",
    "\t\t\t\t['z'],\n",
    "\t\t\t\t['r','x','n','o','s'],\n",
    "\t\t\t\t['y','r','x','z','q','t','p'],\n",
    "\t\t\t\t['y','z','x','e','q','s','t','m']]\n",
    "\treturn simpDat\n",
    "\n",
    "def createInitSet(dataSet):\n",
    "\tretDict = {}\n",
    "\tfor trans in dataSet:\n",
    "\t\tretDict[frozenset(trans)] = 1\n",
    "\treturn retDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpDat = loadSimpDat()\n",
    "simpDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initSet = createInitSet(simpDat)\n",
    "initSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFPtree, myHeaderTab = createTree(initSet,3)\n",
    "myFPtree.disp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCY\n",
    "The **Park-Chen-Yu algorithm**, also called PCY algorithm, addresses one of the limitations of the Apriori algorithm. It tries to limit the computationnal resources needed for the second pass through the database. This is done via hashing.\n",
    "### Observation\n",
    "During the first pass of the Apriori algorithm through the database, most of the memory is idle. We could use the rest to help shorten the list of itemsets of size 2 that we will have to check during the second pass. We could achieve this by **hashing** pairs (i.e. itemsets of size 2), we encouter in each transaction, to buckets. Each time a pair hashes to a bucket, we increment this bucket's count of 1. Note that it is not the same as counting occurrences of each pair for two reasons:\n",
    "1. because there might be collisions (i.e. two different pairs hashing to the same buckets);\n",
    "2. because we don't remember which pair hashes to which bucket.\n",
    "\n",
    "Then, given the \"count\" of each bucket, we can filter more aggressively the list of candidates of size 2 in the Pruning Step. Indeed, if a candidate pair does not hash to a bucket that has a \"count\" greater than, or equal to the support threshold, then this pair won't be frequent as well. However keep in mind that even with this additionnal filter, we might still have candidates that are not frequent hence we still need to pass through the database once more to filter these out. But we will have to check less pairs in the second pass, which is the more computationally intensive one.<br>\n",
    "Let's illustrate these last points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(freq_itemsets):\n",
    "    k = len(freq_itemsets[0]) + 1\n",
    "    new_candidates = []\n",
    "    if k == 2:\n",
    "        for i, itemseti in enumerate(freq_itemsets[:-1]):\n",
    "            for itemsetj in freq_itemsets[i+1:]:\n",
    "                new_candidates.append(tuple(itemseti + itemsetj))\n",
    "    else:\n",
    "        for i, itemseti in enumerate(freq_itemsets[:-1]):\n",
    "            for itemsetj in freq_itemsets[i+1:]:\n",
    "                *itemseti_first, itemseti_last = itemseti\n",
    "                *itemsetj_first, itemsetj_last = itemsetj\n",
    "                if itemseti_first == itemsetj_first:\n",
    "                    new_candidates.append(tuple(itemseti_first + [itemseti_last] + [itemsetj_last]))\n",
    "    return new_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfunction(itemset, num_buckets):\n",
    "    return sum([ord(element) for element in str(itemset)]) % num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [[1,4], [1,5], [2,3]]\n",
    "for pair in pairs:\n",
    "    print('The pair {} hashes to bucket #{}'.format(pair, hashfunction(pair, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy example, we only requested a hash function that created 3 buckets. It is then obvious that multiple pairs will hash to the same bucket and thus collide, as it is the case for pairs \\[1, 4\\], and \\[2, 3\\]. Hence if multiple pairs which individually are not frequent hash to the same bucket, inflating its \"count\" to the point where it's greater than the support threshold then we would have to check all those pairs in the second pass to discover that they are not frequent. Hence, it is important that the hash function reduces collisions as much as possible by evenly distributing pairs among the buckets. On the other hand, we don't want too many buckets because we don't want our hash table to take too much space in memory. Furthermore, to save space, this hash table will be saved as a bitmap. This is illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtable(candidates):\n",
    "    hashtbl = {}\n",
    "    num_buckets = 11  # Arbitrary choice, could be adjusted dynamically\n",
    "    for candidate in candidates:\n",
    "        hashvalue = hashfunction(candidate, num_buckets)\n",
    "        hashtbl[hashvalue] = hashtbl.get(hashvalue, 0) + 1\n",
    "    return hashtbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitmap(hashtable, num_buckets, support_threshold):\n",
    "    return [1 if hashtable.get(i, 0) >= support_threshold else 0 for i in range(num_buckets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_pass_and_hash(database_file, support_threshold):\n",
    "    raw_count = {}\n",
    "    pairs = []\n",
    "    with open(database_file) as database:\n",
    "        for transaction in database:\n",
    "            transaction = set(transaction[:-1].split(','))  # Creating a set to avoid counting multiple times a duplicated item from a basket\n",
    "            for itemset in transaction:\n",
    "                itemset = (itemset,)\n",
    "                raw_count[itemset] = raw_count.get(itemset, 0) + 1\n",
    "            for i, itemseti in enumerate(list(transaction)[:-1]):\n",
    "                for itemsetj in list(transaction)[i+1:]:\n",
    "                    pairs.append(tuple([itemseti] + [itemsetj]))\n",
    "            bitmap_hashtable = bitmap(hashtable(pairs), 11, support_threshold)\n",
    "\n",
    "    return raw_count, bitmap_hashtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data =  'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/Generic_Transactions_db.txt'\n",
    "DataSet = wget.download(link_to_data)\n",
    "\n",
    "\n",
    "database_file = '/content/Generic_Transactions_db.txt'  # Feel free to try the different databases from the dataset folder\n",
    "raw_count, bitmap_hashtable = first_pass_and_hash(database_file, 2)\n",
    "print('Raw count of itemsets of size 1:\\n{}\\n\\nBitmap:\\n{}'.format(raw_count, bitmap_hashtable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that python has a hash function already implemented. The one used here is to showcase the fact that even if the hash function produces collisions, the algorithm will still work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We now have everything to implement this PCY algorithm. Just bear in mind that:\n",
    "- this hashing trick is only used between the first and second passes but not on subsequent passes;\n",
    "- the bitmap of the hash table is used in the Prune Step\n",
    "- for a pair \\[$i$, $j$\\] to remain in the set of candidates for the second pass:\n",
    "    - both $i$ and $j$ need to be frequent\n",
    "    - the pair \\[$i$, $j$\\] needs to hash to a bucket with \"count\" $\\geq$ support threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(candidates, freq_itemsets, bitmap):\n",
    "    kept_candidates = []\n",
    "    for candidate in candidates:\n",
    "        if bitmap is None or bitmap[hashfunction(candidate, 11)] == 1:\n",
    "            for i in range(len(candidate) - 2):\n",
    "                subset = candidate[:i] + candidate[i + 1 :]\n",
    "                if subset not in freq_itemsets:\n",
    "                    break\n",
    "            else:\n",
    "                kept_candidates.append(candidate)\n",
    "    return kept_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcy(database_file, support_threshold):\n",
    "    raw_count, bitmap = first_pass_and_hash(database_file, support_threshold)\n",
    "    first_pass = True\n",
    "    candidates = raw_count.keys()\n",
    "    vault_freq_itemsets = []\n",
    "    \n",
    "    while len(candidates) > 0:\n",
    "        freq_itemsets = []\n",
    "        \n",
    "        if not first_pass:\n",
    "            with open(database_file) as database:\n",
    "                for transaction in database:\n",
    "                    transaction = set(transaction[:-1].split(','))  # Creating a set to avoid counting multiple times a duplicated item from a basket\n",
    "                    for candidate in candidates:\n",
    "                        if set(candidate).issubset(transaction):\n",
    "                            raw_count[candidate] = raw_count.get(candidate, 0) + 1\n",
    "        \n",
    "        freq_itemsets = [itemset for itemset, support in raw_count.items() if support >= support_threshold]\n",
    "        vault_freq_itemsets.append(freq_itemsets)\n",
    "                            \n",
    "        candidates = join(freq_itemsets)\n",
    "        \n",
    "        candidates = prune(candidates, freq_itemsets, bitmap)\n",
    "        \n",
    "        if first_pass:\n",
    "            first_pass = False\n",
    "            bitmap = None\n",
    "        raw_count = {}\n",
    "        \n",
    "    return vault_freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = pcy(database_file, 2)\n",
    "for size, frequent_itemset in enumerate(frequent_itemsets):\n",
    "    print('Frequent itemsets of size {}:\\n{}'.format(size+1, frequent_itemset),end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "It is important to notice that the PCY algorithm is an extension to the Apriori algorithm. It partially addresses one of the issues of the Apriori algorithm, more specifically the heavy workload necessary to find frequent itemsets of size 2. This bottleneck of the Apriori algorithm is minimized by cleverly using a hashing technique. This technique provides a new filter that is used at the end of the first pass to reduce the size of the set of candidate pairs that will have to be checked during the second pass.<br>\n",
    "Note however that the efficiency of this technique depends heavily on the hash function. In the extreme case where all pairs are hashed to the same bucket, the PCY extension does not provide any help since it won't filter out any candidates.<br>\n",
    "This PCY algorithm does not propose any solution to the fact that we still have to go through the database multiple times to find frequent itemsets of increasing size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approaches\n",
    "Let's now turn our attention to other algorithms/extensions. We will briefly introduce them and refer you to other sources to get more information.\n",
    "### Multihash algorithm\n",
    "In the continuity of the PCY algorithm, this one proposes to hash, during the first pass, all pairs but **multiple times with different hash functions**. This is done in the hope that even if two different unfrequent pairs hash to the same bucket, with the first hash function, such that they both have to be kept as candidates because the bucket's \"count\" $\\geq$ support threshold, the second hash function will hash them to different buckets, thus increasing the chance that they will be filtered out. One might then want to use many hash functions but due to the limited size of the memory, more hash functions means that each one will have a restricted space. Thus the more hash functions you have, the less buckets each of them will be able to have, increasing then the chance of collisions. There is thus clearly a tradeoff to be made.<br><br>\n",
    "Source to go further: __[Mining of Massive Datasets](http://www.mmds.org/)__\n",
    "### Toivonen's algorithm\n",
    "This algorithm attempts to limit the number of passes needed to find frequent items. In the best case scenario, this algorithm will only need two passes through the database to find all frequent itemsets (of any size $s$). However, this comes with a cost: the algorithm is not guaranteed to return a result at each run. Hence it might be necessary to rerun it multiple times. During it's first pass, Toivonen's algorithm will use a sample of the database and find all frequent itemsets that are in it, according to a lowered support threshold (set according to the sample's size). Next to those frequent itemsets, the algorithm also constructs a list called **Negative Border** which contains all itemsets not deemed frequent in the sample but for which all its immediate subsets are. During the second pass, the algorithm uses the full database and checks which of the frequent itemsets from the sample are also frequent in the database. Moreover, during this last pass, it checks that none of the itemsets from the Negative Border is frequent in the database. If this is the case, then the algorithm has to be rerun because it might be that one frequent itemset has not been discovered. In the other case, the algorithm returns the list of frequent itemsets.<br><br>\n",
    "Source to go further: __[Mining of Massive Datasets](http://www.mmds.org/)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have seen multiple approaches that can be used to mine frequent itemsets from a database of transactions.<br>\n",
    "The extensions that we have covered, such as the hashing technique, and the sampling of the database, are neat tricks that can often be used in other contexts and scenario. This is why it is often useful to keep in mind tricks that helped speed up algorithms in other areas because they can often be applied to algorithms from other fields. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
