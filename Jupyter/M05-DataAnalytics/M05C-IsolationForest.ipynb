{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cloud-First](../figures/CloudFirst.png) \n",
    "\n",
    "\n",
    "# SIT742: Big Data Analytics \n",
    "**(Module: Data Analytics)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Session 5C: Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the required libraries first. We are importing numpy, pandas, seaborn and matplotlib. Apart form that we also need to import IsolationForest from sklearn.ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the libraries are imported we need to read the data from the csv to the pandas data frame and check the first 10 rows of data.\n",
    "\n",
    "The data is a collection of XXXX. This data has few anomalies (like values too high or too low) which we will be detecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/XXXX.csv'\n",
    "DataSet = wget.download(link_to_data)\n",
    "\n",
    "# Data Preprocessing\n",
    "df = pd.read_csv('XXXX.csv', header = None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "To get more of an idea of the data we have plotted a violin plot of salary data as shown below. A violin plot is a method of plotting numeric data.\n",
    "\n",
    "Typically a violin plot includes all the data that is in a box plot, a marker for the median of the data, a box or marker indicating the interquartile range, and possibly all sample points, if the number of samples is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for visualization etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of outliers we may like to look at a box plot as well. This is also known as box-and-whisker plot. The box in box plot shows the quartiles of the dataset, while the whiskers shows the rest of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IsolationForest\n",
    "\n",
    "Once we have completed our exploratory data analysis, it's time to define and fit the model.\n",
    "\n",
    "We'll create a model variable and instantiate the IsolationForest class. We are passing the values of four parameters to the Isolation Forest method, listed below.\n",
    "\n",
    "- Number of estimators: n_estimators refers to the number of base estimators or trees in the ensemble, i.e. the number of trees that will get built in the forest. This is an integer parameter and is optional. The default value is 100.\n",
    "\n",
    "- Max samples: max_samples is the number of samples to be drawn to train each base estimator. If max_samples is more than the number of samples provided, all samples will be used for all trees. The default value of max_samples is 'auto'. If 'auto', then max_samples=min(256, n_samples)\n",
    "\n",
    "- Contamination: This is a parameter that the algorithm is quite sensitive to; it refers to the expected proportion of outliers in the data set. This is used when fitting to define the threshold on the scores of the samples. The default value is 'auto'. If ‘auto’, the threshold value will be determined as in the original paper of Isolation Forest.\n",
    "\n",
    "- Max features: All the base estimators are not trained with all the features available in the dataset. It is the number of features to draw from the total features to train each base estimator or tree.The default value of max features is one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.1),max_features=1.0)\n",
    "\n",
    "model.fit(df[['XXX']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we defined the model above we need to train the model using the data given. For this we are using the fit() method as shown above. This method is passed one parameter, which is our data of interest (in this case, the salary column of the dataset).\n",
    "\n",
    "Once the model is trained properly it will output the IsolationForest instance as shown in the output of the cell above.\n",
    "\n",
    "### Anomaly Score\n",
    "\n",
    "Now this is the time to add the scores and anomaly column of the dataset.\n",
    "\n",
    "After the model is defined and fit, let's find the scores and anomaly column. We can find out the values of scores column by calling `decision_function()` of the trained model and passing the salary as parameter.\n",
    "\n",
    "Similarly we can find the values of anomaly column by calling the `predict()` function of the trained model and passing the salary as parameter.\n",
    "\n",
    "These columns are going to be added to the data frame df. After adding these two columns let's check the data frame. As expected, the data frame has three columns now: salary, scores and anomaly. A negative score value and a -1 for the value of anomaly columns indicate the presence of anomaly. A value of 1 for the anomaly represents the normal data.\n",
    "\n",
    "Each data point in the train set is assigned an anomaly score by this algorithm. We can define a threshold, and using the anomaly score, it may be possible to mark a data point as anomalous if its score is greater than the predefined threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores']=model.decision_function(df[['XXX']])\n",
    "df['anomaly']=model.predict(df[['XXX']])\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Anomalies\n",
    "\n",
    "After adding the scores and anomalies for all the rows in the data, we will print the predicted anomalies. To print the predicted anomalies in the data we need to analyse the data after addition of scores and anomaly column. As you can see above for the predicted anomalies the anomaly column values would be -1 and their scores will be negative. Using this information we can print the predicted anomaly (two data points in this case) as below.\n",
    "\n",
    "Note that we could print not only the anomalous values but also their index in the dataset, which is useful information for further processing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly=df.loc[df['anomaly']==-1]\n",
    "anomaly_index=list(anomaly.index)\n",
    "print(anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "For evaluating the model let's set a threshold as salary > 99999 is an outlier.Let us find out the number of outlier present in the data as per the above rule using code as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_counter = len(df[df['XXX'] > 99999]) % some value?\n",
    "outliers_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the accuracy of the model by finding how many outlier the model found divided by how many outliers present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy percentage:\", 100*list(df['anomaly']).count(-1)/(outliers_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy percentage: 100 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## IsolationForest example\n",
    "\n",
    "\n",
    "An example using IsolationForest for anomaly detection.\n",
    "\n",
    "The IsolationForest 'isolates' observations by randomly selecting a feature\n",
    "and then randomly selecting a split value between the maximum and minimum\n",
    "values of the selected feature.\n",
    "\n",
    "Since recursive partitioning can be represented by a tree structure, the\n",
    "number of splittings required to isolate a sample is equivalent to the path\n",
    "length from the root node to the terminating node.\n",
    "\n",
    "This path length, averaged over a forest of such random trees, is a measure\n",
    "of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeable shorter paths for anomalies.\n",
    "Hence, when a forest of random trees collectively produce shorter path lengths\n",
    "for particular samples, they are highly likely to be anomalies.\n",
    "\n",
    ".. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n",
    "    Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate train data\n",
    "X = 0.3 * rng.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * rng.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = IsolationForest(max_samples=100, random_state=rng)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "\n",
    "# plot the line, the samples, and the nearest vectors to the plane\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',\n",
    "                 s=20, edgecolor='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',\n",
    "                 s=20, edgecolor='k')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',\n",
    "                s=20, edgecolor='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([b1, b2, c],\n",
    "           [\"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
