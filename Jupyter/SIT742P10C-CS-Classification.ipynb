{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# SIT742: Modern Data Science \n**(Week 10: Data Analytics (III))**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n\nPrepared by **SIT742 Teaching Team**\n\n---\n\n\n\n## Session 10C - Spark MLlib (5): Case Study - Classification", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Apache Spark, once a component of the Hadoop ecosystem, is now becoming the big-data platform of choice for enterprises. It is a powerful open source engine that provides real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and standard interface.\n\nIn the industry, there is a big demand for a powerful engine that can do all of above. Sooner or later, your company or your clients will be using Spark to develop sophisticated models that would enable you to discover new opportunities or avoid risk. Spark is not hard to learn, if you already known Python and SQL, it is very easy to get started. Let\u2019s give it a try today!", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Binary Classification with PySpark and MLlib", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Exploring the Data\n\nWe will use the same data set when we built a Logistic Regression in Python, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (Yes/No) to a term deposit. The dataset can be downloaded from Kaggle.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install wget\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n!wget -q https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n!pip install -q findspark\n\nimport os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import findspark\nfindspark.init()\nfrom pyspark.sql import SparkSession "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/bank.csv'\nbank = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\ndf = spark.read.csv(bank, header = True, inferSchema = True)\ndf.printSchema()"
        }, 
        {
            "source": "Input variables: age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome.\n\nOutput variable: deposit\n\nHave a peek of the first five observations. Pandas data frame is prettier than Spark DataFrame.show().", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\npd.DataFrame(df.take(5), columns=df.columns).transpose()"
        }, 
        {
            "source": "Our classes are perfect balanced.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\npd.DataFrame(df.take(2), columns=df.columns).transpose()"
        }, 
        {
            "source": "Summary statistics for numeric variables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\ndf.select(numeric_features).describe().toPandas().transpose()"
        }, 
        {
            "source": "Correlations between independent variables.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "numeric_data = df.select(numeric_features).toPandas()\naxs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\n\nn = len(numeric_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())"
        }, 
        {
            "source": "It\u2019s obvious that there aren\u2019t highly correlated numeric variables. Therefore, we will keep all of them for the model. However, day and month columns are not really useful, we will remove these two columns.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df = df.select(\"age\",\"job\",\"marital\",\"education\",\"default\",\"balance\",\"housing\",\"loan\",\"contact\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\",\"deposit\")\ncols = df.columns\ndf.printSchema()"
        }, 
        {
            "source": "## Preparing Data for Machine Learning\n\nThe process includes Category Indexing, One-Hot Encoding and VectorAssembler\u200a\u2014\u200aa feature transformer that merges multiple columns into a vector column.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\nstages = []"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for categoricalCol in categoricalColumns:\n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    stages += [stringIndexer, encoder]\nlabel_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\nstages += [label_stringIdx]\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"
        }, 
        {
            "source": "The above code are taken from databricks\u2019 official site and it indexes each categorical column using the StringIndexer, then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row. We use the StringIndexer again to encode our labels to label indices. Next, we use the VectorAssembler to combine all the feature columns into a single vector column.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Pipeline\n\nWe use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow. A Pipeline\u2019s stages are specified as an ordered array.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\nselectedCols = ['label', 'features'] + cols\ndf = df.select(selectedCols)\ndf.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
        }, 
        {
            "source": "As you can see, we now have features column and label column.\n\nRandomly split data into train and test sets, and set seed for reproducibility.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))"
        }, 
        {
            "source": "## Logistic Regression Model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)"
        }, 
        {
            "source": "We can obtain the coefficients by using LogisticRegressionModel\u2019s attributes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nbeta = np.sort(lrModel.coefficients)\n\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()"
        }, 
        {
            "source": "Summarize the model over the training set, we can also obtain the receiver-operating characteristic and areaUnderROC.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "trainingSummary = lrModel.summary\n\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
        }, 
        {
            "source": "Precision and recall.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()"
        }, 
        {
            "source": "Make predictions on the test set.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "predictions = lrModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
        }, 
        {
            "source": "Evaluate our Logistic Regression model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', evaluator.evaluate(predictions))"
        }, 
        {
            "source": "## Decision Tree Classifier\n\nDecision trees are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\ndtModel = dt.fit(train)\npredictions = dtModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
        }, 
        {
            "source": "Evaluate our Decision Tree model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
        }, 
        {
            "source": "One simple decision tree performed poorly because it is too weak given the range of different features. The prediction accuracy of decision trees can be improved by Ensemble methods, such as Random Forest and Gradient-Boosted Tree.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Random Forest Classifier", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\nrfModel = rf.fit(train)\npredictions = rfModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
        }, 
        {
            "source": "Evaluate our Random Forest Classifier.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
        }, 
        {
            "source": "## Gradient-Boosted Tree Classifier", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier(maxIter=10)\ngbtModel = gbt.fit(train)\npredictions = gbtModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
        }, 
        {
            "source": "Evaluate our Gradient-Boosted Tree Classifier.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
        }, 
        {
            "source": "Gradient-Boosted Tree achieved the best results, we will try tuning this model with the ParamGridBuilder and the CrossValidator. Before that we can use explainParams() to print a list of all params and their definitions to understand what params available for tuning.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(gbt.explainParams())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 60])\n             .addGrid(gbt.maxIter, [10, 20])\n             .build())\ncv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\ncvModel = cv.fit(train)\npredictions = cvModel.transform(test)\nevaluator.evaluate(predictions)"
        }, 
        {
            "source": "# Multi-Class Text Classification with PySpark", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Apache Spark is quickly gaining steam both in the headlines and real-world adoption, mainly because of its ability to process streaming data. With so much data being processed on a daily basis, it has become essential for us to be able to stream and analyze it in real time. In addition, Apache Spark is fast enough to perform exploratory queries without sampling. Many industry experts have provided all the reasons why you should use Spark for Machine Learning?\n\nSo, here we are now, using Spark Machine Learning Library to solve a multi-class text classification problem, in particular, PySpark.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## The Data \n\nOur task is to classify San Francisco Crime Description into 33 pre-defined categories. The data can be downloaded from [Kaggle](https://www.kaggle.com/c/sf-crime/data).\n\nGiven a new crime description comes in, we want to assign it to one of 33 categories. The classifier makes the assumption that each new crime description is assigned to one and only one category. This is multi-class text classification problem.\n\n\n* Input: Descript\n\nExample: \u201c STOLEN AUTOMOBILE\u201d\n\n* Output: Category\n\nExample: VEHICLE THEFT\n\nTo solve this problem, we will use a variety of feature extraction technique along with different supervised machine learning algorithms in Spark. Let\u2019s get started!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Data Ingestion and Extraction", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark import SparkContext\nfrom pyspark import SparkConf\nfrom pyspark.sql import SQLContext\n\nconf = SparkConf().setAppName('project1').setMaster('local')\nsc = SparkContext.getOrCreate(conf)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# import wget\n\n# link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/train.csv'\n\n# train = wget.download(link_to_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# from pyspark.sql import SQLContext\n# from pyspark import SparkContext\n# sc =SparkContext()\nsqlContext = SQLContext(sc)\ndata = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"/FileStore/tables/5ydsu8rv1497528269221/train.csv\")\n# data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(train)"
        }, 
        {
            "source": "That\u2019s it! We have loaded the dataset. Let\u2019s start exploring.\n\nRemove the columns we do not need and have a look the first five rows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\ndata = data.select([column for column in data.columns if column not in drop_list])\ndata.show(5)"
        }, 
        {
            "source": "Apply printSchema() on the data which will print the schema in a tree format:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data.printSchema()"
        }, 
        {
            "source": "Top 20 crime categories:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import col\ndata.groupBy(\"Category\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()"
        }, 
        {
            "source": "Top 20 crime descriptions:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data.groupBy(\"Descript\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()"
        }, 
        {
            "source": "### Model Pipeline\n\nSpark Machine Learning Pipelines API is similar to Scikit-Learn. Our pipeline includes three steps:\n1. `regexTokenizer`: Tokenization (with Regular Expression)\n2. `stopwordsRemover`: Remove Stop Words\n3. `countVectors`: Count vectors (\u201cdocument-term vectors\u201d)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\nfrom pyspark.ml.classification import LogisticRegression\n\n# regular expression tokenizer\nregexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")\n\n# stop words\nadd_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \nstopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n\n# bag of words count\ncountVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
        }, 
        {
            "source": "### StringIndexer\n\n`StringIndexer` encodes a string column of labels to a column of label indices. The indices are in `[0, numLabels)`, ordered by label frequencies, so the most frequent label gets index `0`.\n\nIn our case, the label column (Category) will be encoded to label indices, from 0 to 32; the most frequent label (LARCENY/THEFT) will be indexed as 0.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nlabel_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n\npipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n\n# Fit the pipeline to training documents.\npipelineFit = pipeline.fit(data)\ndataset = pipelineFit.transform(data)\ndataset.show(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "### Partition Training & Test sets\n\n# set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint(\"Training Dataset Count: \" + str(trainingData.count()))\nprint(\"Test Dataset Count: \" + str(testData.count()))"
        }, 
        {
            "source": "## Model Training and Evaluation\n### Logistic Regression using Count Vector Features\n\nOur model will make predictions and score on the test set; we then look at the top 10 predictions from the highest probability.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\nlrModel = lr.fit(trainingData)\n\npredictions = lrModel.transform(testData)\n\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"
        }, 
        {
            "source": "### Logistic Regression using TF-IDF Features", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import HashingTF, IDF\n\nhashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\npipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n\npipelineFit = pipeline.fit(data)\ndataset = pipelineFit.transform(data)\n\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nlr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\nlrModel = lr.fit(trainingData)\n\npredictions = lrModel.transform(testData)\n\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"
        }, 
        {
            "source": "### Cross-Validation\n\nLet\u2019s now try cross-validation to tune our hyper parameters, and we will only tune the count vectors Logistic Regression.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n\npipelineFit = pipeline.fit(data)\ndataset = pipelineFit.transform(data)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n\nlr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n             .build())\n\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, \\\n                    estimatorParamMaps=paramGrid, \\\n                    evaluator=evaluator, \\\n                    numFolds=5)\n\ncvModel = cv.fit(trainingData)\n\npredictions = cvModel.transform(testData)\n\n# Evaluate best model\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"
        }, 
        {
            "source": "## Naive Bayes", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import NaiveBayes\n\nnb = NaiveBayes(smoothing=1)\nmodel = nb.fit(trainingData)\n\npredictions = model.transform(testData)\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"
        }, 
        {
            "source": "## Random Forest", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(labelCol=\"label\", \\\n                            featuresCol=\"features\", \\\n                            numTrees = 100, \\\n                            maxDepth = 4, \\\n                            maxBins = 32)\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)\n\npredictions = rfModel.transform(testData)\n\npredictions.filter(predictions['prediction'] == 0) \\\n    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n    .orderBy(\"probability\", ascending=False) \\\n    .show(n = 10, truncate = 30)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "name": "SIT742P10C-CS-Classification", 
        "notebookId": 2873192807567519
    }, 
    "nbformat": 4
}