{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# SIT742: Modern Data Science \n**(Week 03A: Data Wrangling)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n\nPrepared by **SIT742 Teaching Team**\n\n---\n\n# Session 03A - DataWrangling with Pandas \n\n## Table of Content\n\n* Part 1. Scraping data from the web\n* Part 2. States and Territories of Australia\n* Part 3. Parsing XML files with BeautifulSoup\n\n\n\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Part 1. Scraping data from the web", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Many of you will probably be interested in scraping data from the web for your projects. For example, what if we were interested in working with some historical Canadian weather data? Well, we can get that from: http://climate.weather.gc.ca using their API. Requests are going to be formatted like this:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "url_template = \"http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=5415&Year={year}&Month={month}&timeframe=1&submit=Download+Data\"", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Note that we've requested the data be returned as a csv, and that we're going to supply the month and year as inputs when we fire off the query. To get the data for March 2012, we need to format it with month=3, year=2012:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "url = url_template.format(month=3, year=2012)\nurl", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "This is great! We can just use the same read_csv function as before, and just give it a URL as a filename. Awesome.\n\nUpon inspection, we find out that there are 16 rows of metadata at the top of this CSV, but pandas knows CSVs are weird, so there's a skiprows options. We parse the dates again, and set 'Date/Time' to be the index column. Here's the resulting dataframe.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "weather_mar2012 = pd.read_csv(url, skiprows=15, index_col='Date/Time', parse_dates=True, encoding='latin1')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "weather_mar2012.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "As before, we can get rid of any comlumns that don't contain real data using ${\\tt .dropna()}$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "weather_mar2012 = weather_mar2012.dropna(axis=1, how='any')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "weather_mar2012.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Getting better! The Year/Month/Day/Time columns are redundant, though, and the Data Quality column doesn't look too useful. Let's get rid of those.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "weather_mar2012 = weather_mar2012.drop(['Year', 'Month', 'Day', 'Time'], axis=1)\nweather_mar2012[:5]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Great! Now let's figure out how to download the whole year? It would be nice if we could just send that as a single request, but like many APIs this one is limited to prevent people from hogging bandwidth. No problem: we can write a function!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def download_weather_month(year, month):\n    url = url_template.format(year=year, month=month)\n    weather_data = pd.read_csv(url, skiprows=15, index_col='Date/Time', parse_dates=True)\n    weather_data = weather_data.dropna(axis=1)\n    weather_data.columns = [col.replace('\\xb0', '') for col in weather_data.columns]\n    weather_data = weather_data.drop(['Year', 'Day', 'Month', 'Time'], axis=1)\n    return weather_data", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now to test that this function does the right thing:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "download_weather_month(2012, 1).head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Woohoo! Now we can iteratively request all the months using a single line. This will take a little while to run.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "data_by_month = [download_weather_month(2012, i) for i in range(1, 12)]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Once that's done, it's easy to concatenate all the dataframes together into one big dataframe using ${\\tt pandas.concat()}$. And now we have the whole year's data!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "weather_2012 = pd.concat(data_by_month)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "This thing is long, so instead of printing out the whole thing, I'm just going to print a quick summary of the ${\\tt DataFrame}$ by calling ${\\tt .info()}$:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "weather_2012.info()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "And a quick reminder, if we wanted to save that data to a file:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "weather_2012.to_csv('weather_2012.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!ls", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "And finally, something you should do early on in the wrangling process, plot data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# plot that data\nimport matplotlib.pyplot as plt \n# so now 'plt' means matplotlib.pyplot", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plt.plot(df)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# nothing to see... in iPython you need to specify where the chart will display, usually it's in a new window\n# to see them 'inline' use:\n%matplotlib inline", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plt.plot(df)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# that's better, try other plots, scatter is popular, also boxplot", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Part 2. States and Territories of Australia \n\nWe are interested in getting  State and Territory information from Wikipedia, however we do not want to copy and paste the table : )\n\nHere is the URL\nhttps://en.wikipedia.org/wiki/States_and_territories_of_Australia   \n\nWe need two libraries to do the task:\n\nCheck documentations here:\n* [urllib](https://docs.python.org/2/library/urllib.html)\n* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import sys\nif sys.version_info[0] == 3:\n    from urllib.request import urlopen\nelse:\n    from urllib import urlopen\nfrom bs4 import BeautifulSoup", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We first save the link in wiki", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "wiki = \"https://en.wikipedia.org/wiki/States_and_territories_of_Australia\"", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Then use ulropen to open the page. \n\nIf you get \"SSL: CERTIFICATE_VERIFY_FAILED\", what you need to do is find where \"Install Certificates.command\" file is, and click it to upgrade the certificate. Then, you should be able to solve the problem.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "page = urlopen(wiki)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "if sys.version_info[0] == 3:\n    page = page.read()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You will meet BeautifulSoup later in this subject, so don't worry if you feel uncomfortable with it now. You can always revisit. \n\nWe begin by reading in the source code and creating a Beautiful Soup object with the BeautifulSoup function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "soup = BeautifulSoup(page, \"lxml\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Then we print and see. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " print (soup.prettify())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "For who do not know much about HTML, this might be a bit overwhelming, but essentially it contains lots of tags in the angled brackets providing structural and formatting information that we don't care so much here. What we need is the table. \n\nLet's first check the title.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "soup.title.string", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It looks fine, then we would like to find the table. \n\nLet's have a try to extract all contents within the 'table' tag.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "all_tables = soup.findAll('table')\nprint(all_tables)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "This returns a collection of tag objects. It seems that most of the information are useless and it's getting hard to hunt for the table. So searched online and found an instruction here: \n\nhttps://adesquared.wordpress.com/2013/06/16/using-python-beautifulsoup-to-scrape-a-wikipedia-table/\n\nThe class is \"wikitable sortable\"!! Have a try then.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "right_table=soup.find('table', class_='wikitable sortable')\nprint (right_table)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Next we need to extract table header row by find the first 'tr'>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "head_row = right_table.find('tr')\nprint (head_row)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Then we extract header row name via iterate through each row and extract text. \n\nThe .findAll function in Python returns a list containing all the elements, which you can iterate through.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "header_list = []\nheaders = head_row.findAll('th')\nfor header in headers:\n    #print header.find(text = True)\n    header_list.append(header.find(text = True))\nheader_list", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We can probably iterate trough this list and then extract contents. But let's take a simple approach of extracting each column separately. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "flag=[]\nstate=[]\nabbrev = []\nISO = []\nPostal =[]\nType = []\nCapital = []\npopulation = []\nArea = []\nfor row in right_table.findAll(\"tr\"):\n    cells = row.findAll('td')\n    if len(cells) > 0 and len(cells) == 9:\n        flag.append(cells[0].find(text=True))\n        state.append(cells[1].find(text=True))\n        abbrev.append(cells[2].find(text=True))\n        ISO.append(cells[3].find(text=True))\n        Postal.append(cells[4].find(text=True))\n        Type.append(cells[5].find(text=True))\n        Capital.append(cells[6].find(text=True))\n        population.append(cells[7].find(text=True))\n        Area.append(cells[8].find(text=True))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Next we can append all list to the dataframe.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_au = pd.DataFrame()\ndf_au[header_list[0]] = flag\ndf_au[header_list[1]] = state\ndf_au[header_list[2]]=abbrev\ndf_au[header_list[3]]=ISO\ndf_au[header_list[4]]=Postal\ndf_au[header_list[5]]=Type\ndf_au[header_list[6]]=Capital\ndf_au[header_list[7]]=population\ndf_au[header_list[8]]=Area", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Done !", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_au", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Part 3. Parsing XML files with BeautifulSoup\n\nNow, we are going to demonstrate how to use BeautifulSoup to extract information from the XML file, called \"Melbourne_bike_share.xml\". \n\nFor the documentation of BeautifulSoup, please refer to it <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\">official website</a>. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install wget", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/Melbourne_bike_share.xml'\n\nDataSet = wget.download(link_to_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!ls", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from bs4 import BeautifulSoup\nbtree = BeautifulSoup(open(\"Melbourne_bike_share.xml\"),\"lxml-xml\") ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You can alo print out the Beautifulsoup object by calling the <font color=\"blue\">prettify()</font> function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(btree.prettify())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is easy to figure out information we would like to extract is stored in the following tags\n<ul>\n<li>id </li>\n<li>featurename </li>\n<li>terminalname </li>\n<li>nbbikes </li>\n<li>nbemptydoc </li>\n<li>uploaddate </li>\n<li>coordinates </li>\n</ul>\n\nEach record is stored in \"<row> </row>\". To extract information from those tags, except for \"coordinates\", we use the <font color=\"blue\">find_all()</font> function. Its documentation can be found <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\">here</a>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "featuretags = btree.find_all(\"featurename\")\nfeaturetags", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The output shows that the <font color=\"blue\"> find_all() </font> returns all the 50 station names. Now, we need to exclude the tags and just keep the text stored between the tags.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "for feature in featuretags:\n    print (feature.string)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now, we can put all the above code together using list comprehensions. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "featurenames = [feature.string for feature in btree.find_all(\"featurename\")]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "featurenames", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Similarly, we can use the <font color = \"blue\">find_all()</font> function to extract the other information.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "nbbikes = [feature.string for feature in btree.find_all(\"nbbikes\")]\nnbbikes", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "NBEmptydoc = [feature.string for feature in btree.find_all(\"nbemptydoc\")]\nNBEmptydoc", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "TerminalNames = [feature.string for feature in btree.find_all(\"terminalname\")]\nTerminalNames", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "UploadDate = [feature.string for feature in btree.find_all(\"uploaddate\")]\nUploadDate", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "ids = [feature.string for feature in btree.find_all(\"id\")]\nids", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now, how can we extract the attribute values from the tage called \"coordinates\"?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "lattitudes = [coord[\"latitude\"] for coord in btree.find_all(\"coordinates\")]\nlattitudes", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "longitudes = [coord[\"longitude\"] for coord in btree.find_all(\"coordinates\")]\nlongitudes", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "After the extraction, we can put all the information in a Pandas DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import pandas as pd \ndataDict = {}\ndataDict['Featurename'] = featurenames\ndataDict['TerminalName'] = TerminalNames\ndataDict['NBBikes'] = nbbikes\ndataDict['NBEmptydoc'] = NBEmptydoc\ndataDict['UploadDate'] = UploadDate\ndataDict['lat'] = lattitudes\ndataDict['lon'] = longitudes\ndf = pd.DataFrame(dataDict, index = ids)\ndf.index.name = 'ID'\ndf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "name": "DataWrangling with Python and Pandas.ipynb"
    }, 
    "nbformat": 4
}