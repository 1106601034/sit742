{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbeZa7WS_2-s"
   },
   "source": [
    "# SIT742: Modern Data Science \n",
    "**(Module: Data Manipulation)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Data Wrangling\n",
    "\n",
    "In this session, we will learn how to use Python `pandas` package to do data wrangling.\n",
    "\n",
    "\n",
    "\n",
    "## Content\n",
    "\n",
    "\n",
    "### 1. `Pandas` Basics\n",
    "\n",
    "### 2. Loading `CSV` Data\n",
    "\n",
    "### 3. Data Extraction through Web `API` \n",
    "\n",
    "### 4. Web Crawling using `BeautifulSoup`\n",
    "\n",
    "\n",
    "\n",
    "**Note**: The data available on those service might be changing, so you need to adjust the code to accommodate those changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdDCBCLn_2-1"
   },
   "source": [
    "## Part 1. Using `Pandas` to Load `CSV` Data Sets\n",
    "\n",
    "Here you will learn  how to use Pandas [read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function to load a CSV file. Before we start importing our CSV file, it might be good for you to read [Pandas tutorial on reading CSV files](http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table).\n",
    "\n",
    "If `wget` was not installed in your `Python` platform, install it first:\n",
    "\n",
    "To get started, we can import `Pandas` with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish: Unknown command: pip\r\n",
      "fish: \r\n",
      "pip install wget\r\n",
      "^\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the `csv` data file is avilable at a URL, we use `wget` to download it to the local file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/user_raw1.csv'\n",
    "DataSet = wget.download(link_to_data)\n",
    "\n",
    "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/user_raw2.csv'\n",
    "DataSet = wget.download(link_to_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing `CSV` data\n",
    "\n",
    "Importing `CSV` files with `Pandas` function [`read_csv()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)  and converting the data into a form Python can understand is simple. It only takes a couple of lines of code. The imported data will be stored in Pandas `DataFrame`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdf1 = pd.read_csv(\"user_raw1.csv\")\n",
    "userdf2 = pd.read_csv(\"user_raw2.csv\")\n",
    "\n",
    "\n",
    "userdf1.head()\n",
    "userdf2.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TkOmnCyu_2-5"
   },
   "source": [
    "## Part 2. Structuring Data\n",
    "\n",
    "\n",
    "Change the columns name, so that it is consistent.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04ZKvRtM_2_J"
   },
   "source": [
    "### 2.2 Renaming Column Names as Per Convenience\n",
    "\n",
    "This steps involves renaming the colmns names because many column names are confusing and hard to understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_bhAPpd_2_M"
   },
   "outputs": [],
   "source": [
    "new_name = {'Sex': 'Gender',\n",
    "           'Addr.': 'Address'}\n",
    "\n",
    "userdf1.rename(columns= new_name, inplace = True)\n",
    "\n",
    "new_name = {'Surname': 'Family Name',\n",
    "           'First Name': 'Given Name',\n",
    "           'Addr.': 'Address'}\n",
    "\n",
    "userdf2.rename(columns= new_name, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o76kkUME_2_V"
   },
   "source": [
    "### 2.3 Replacing the value of the rows if needed\n",
    "\n",
    "This involves replacing the values with values more reaable, such as `M` by `Male`, etc. Please complete the code to replace the state abbreviations by its full name, such as `VIC` by `Victoria`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5lP8CF9_2_Y"
   },
   "outputs": [],
   "source": [
    "replace_values = {'M': 'Male', 'F': 'Female'}\n",
    "\n",
    "userdf1 = userdf1.replace({'Gender': replace_values})\n",
    "userdf2 = userdf2.replace({'Gender': replace_values})\n",
    "\n",
    "userdf1.head()\n",
    "userdf2.head()\n",
    "\n",
    "# Your code to replace values for the column Address\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5P7FONd_3CR"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYlnsbPr_3CV"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBmeWNVx_3Ce"
   },
   "source": [
    "## Part 3. Data Cleaning\n",
    "\n",
    "We have seen the selection of \n",
    "\n",
    "### 3.1 Removing the Irrelevant Columns\n",
    "\n",
    "Suppose two irrelevant columns are `PID` and `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['PID', 'target']\n",
    "\n",
    "userdf1.drop(to_drop, inplace=True, axis = 1)\n",
    "userdf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBmeWNVx_3Ce"
   },
   "source": [
    "### 3.2 Missing Data\n",
    "\n",
    "To find and fill the missing data in the dataset we will use another function. There are 4 ways to find the null values if present in the dataset. Let’s see them one by one:\n",
    "\n",
    "- Using `isnull()`: This function provides the boolean value for the complete dataset to know if any null value is present or not.\n",
    "- Using `isna().any()`: This function gives a boolean value if any null value is present or not, but it gives results column-wise, not in tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7gpk9P-_3Cg"
   },
   "outputs": [],
   "source": [
    "\n",
    "# please add code to illustrate drop those null value rows, or drop null value columns, one for one df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAkmBKlD_3E1"
   },
   "source": [
    "### 3.3 Data Merging\n",
    "\n",
    "XXX What to do? to merge them, or to conta\n",
    "\n",
    "Merging the dataset is the process of combining two datasets in one, and line up rows based on some particular or common property for data analysis. We can do this by using the `merge()` function of the dataframe. Following is the syntax of the merge function:\n",
    "\n",
    "Left join? or jst Concanate...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SVh16b8_3E3"
   },
   "outputs": [],
   "source": [
    "userdf1.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 De-Duplicate\n",
    "\n",
    "De-Duplicate means remove all duplicate values. There is no need for duplicate values in data analysis. These values only affect the accuracy and efficiency of the analysis result. To find duplicate values in the dataset we will use a simple dataframe function i.e. `duplicated()`. Let’s see the example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userdf.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function also provides bool values for duplicate values in the dataset. \n",
    "\n",
    "If a dataset contains duplicate values it can be removed using the `drop_duplicates()` function. Following is the syntax of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdf.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Data Transformation\n",
    "\n",
    "\n",
    "Data transformation is a common practice in machine learning. \n",
    "\n",
    "\n",
    "### 4.1 Using The min-max normaization maximum absolute scaling\n",
    "\n",
    "Typically, Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
    "\n",
    "The min-max approach (often called normalization) rescales the feature to a hard and fast range of [0,1] by subtracting the minimum value of the feature then dividing by the range. We can apply the min-max scaling in Pandas using the .min() and .max() methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data\n",
    "df_min_max_scaled = df.copy()\n",
    "  \n",
    "# apply normalization techniques\n",
    "for column in df_min_max_scaled.columns:\n",
    "    df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view normalized data\n",
    "print(df_min_max_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Z-Score Standardization\n",
    "\n",
    "Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
    "\n",
    "The z-score method (often called standardization) transforms the info into distribution with a mean of 0 and a typical deviation of 1. Each standardized value is computed by subtracting the mean of the corresponding feature then dividing by the quality deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data\n",
    "df_z_scaled = df.copy()\n",
    "  \n",
    "# apply normalization techniques\n",
    "for column in df_z_scaled.columns:\n",
    "    df_z_scaled[column] = (df_z_scaled[column] -\n",
    "                           df_z_scaled[column].mean()) / df_z_scaled[column].std()    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view normalized data   \n",
    "display(df_z_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Export Dataset\n",
    "\n",
    "This is the last step of the data cleaning process. After performing all the above operations, the data is transformed into clean the dataset and it is ready to export for the next process in Data Science or Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "s5P7FONd_3CR",
    "5kjxc5gJ_3E-"
   ],
   "name": "SIT742P03A-DataWrangling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
