{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b5067dfe",
      "metadata": {
        "id": "b5067dfe"
      },
      "source": [
        "![Cloud-First](../image/CloudFirst.png)\n",
        "\n",
        "\n",
        "# SIT742: Modern Data Science\n",
        "**(Module: Big Data Manipulation)**\n",
        "\n",
        "---\n",
        "- Materials in this module include resources collected from various open-source online repositories.\n",
        "- You are free to use, change and distribute this package.\n",
        "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
        "\n",
        "\n",
        "Prepared by **SIT742 Teaching Team**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Session 4I: SparkSQL and Data Understanding\n",
        "---\n",
        "\n",
        "### Table of Content\n",
        "\n",
        "Part A: Spark Text Data Foundation\n",
        "\n",
        "1. Word2Vec in Spark\n",
        "\n",
        "2. Tokenization on the Given Document\n",
        "\n",
        "3. Stop Words Removal\n",
        "4. N Gram Tokenization\n",
        "5. CountVectorizer Representation\n",
        "\n",
        "Part B: Adcanced Representation\n",
        "\n",
        "6. Binarizer on the Continuous Feature\n",
        "\n",
        "7. PCA in Spark\n",
        "8.Polynomial Representation For Continuous Data\n",
        "9.Min Max Scaler in Spark\n",
        "10.Discretization in Spark\n",
        "11.Imputation in Spark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction ##\n",
        "\n",
        "This notebook will introduce Spark capabilities to deal with the text data and also continuous data with ML.feature module.\n",
        "The ML.feature module is a powerful module that provides a wide array of feature transformers and tools to prepare data for machine learning in a scalable and distributed manner. It is designed to integrate smoothly into ML pipelines using the pyspark.ml API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Spark Text Data Foundation"
      ],
      "metadata": {
        "id": "QSt_kUId16dD"
      },
      "id": "QSt_kUId16dD"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ranking-father",
      "metadata": {
        "id": "ranking-father"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"Intro\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8340e25",
      "metadata": {
        "id": "e8340e25"
      },
      "source": [
        "Word2Vec in Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "stylish-jimmy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stylish-jimmy",
        "outputId": "6ebbfd5b-a1b0-4af6-f1bd-bb7aea3541dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: [Hi, this, is, from, SIT742] => \n",
            "Vector: [0.010596102662384511,0.017389786010608078,-0.05054501481354237]\n",
            "\n",
            "Text: [I, THINK, sit742, IS, a, good, unit] => \n",
            "Vector: [-0.04077614334944103,-0.004508644342422485,0.052702106512567425]\n",
            "\n",
            "Text: [Spark, is, not, hard, to, learn] => \n",
            "Vector: [0.07659062929451466,-0.015291144760946432,-0.07604554109275341]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "# Input data: Each row is a bag of words from a sentence or document.\n",
        "documentDF = spark.createDataFrame([\n",
        "    (\"Hi this is from SIT742\".split(\" \"), ),\n",
        "    (\"I THINK sit742 IS a good unit\".split(\" \"), ),\n",
        "    (\"Spark is not hard to learn\".split(\" \"), )\n",
        "], [\"text\"])\n",
        "\n",
        "# Learn a mapping from words to Vectors.\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(documentDF)\n",
        "\n",
        "result = model.transform(documentDF)\n",
        "for row in result.collect():\n",
        "    text, vector = row\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization on the Given Document"
      ],
      "metadata": {
        "id": "8P1kHmia3of-"
      },
      "id": "8P1kHmia3of-"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "parallel-quilt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "parallel-quilt",
        "outputId": "7f7d8c7c-0f98-4243-9005-b7d2e6bd558e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+-------------------------------------------------+------+\n",
            "|sentence                           |words                                            |tokens|\n",
            "+-----------------------------------+-------------------------------------------------+------+\n",
            "|Hi|this|is|from|SIT742             |[hi|this|is|from|sit742]                         |1     |\n",
            "|I     THINK sit742  IS a  good unit|[i, , , , , think, sit742, , is, a, , good, unit]|13    |\n",
            "|Spark,is,not,hard,to,learn         |[spark,is,not,hard,to,learn]                     |1     |\n",
            "+-----------------------------------+-------------------------------------------------+------+\n",
            "\n",
            "+-----------------------------------+-------------------------------------+------+\n",
            "|sentence                           |words                                |tokens|\n",
            "+-----------------------------------+-------------------------------------+------+\n",
            "|Hi|this|is|from|SIT742             |[hi, this, is, from, sit742]         |5     |\n",
            "|I     THINK sit742  IS a  good unit|[i, think, sit742, is, a, good, unit]|7     |\n",
            "|Spark,is,not,hard,to,learn         |[spark, is, not, hard, to, learn]    |6     |\n",
            "+-----------------------------------+-------------------------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi|this|is|from|SIT742\"),\n",
        "    (1, \"I     THINK sit742  IS a  good unit\"),\n",
        "    (2, \"Spark,is,not,hard,to,learn\")\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
        "\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "tokenized = tokenizer.transform(sentenceDataFrame)\n",
        "tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
        "\n",
        "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
        "regexTokenized.select(\"sentence\", \"words\") \\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop Words Removal"
      ],
      "metadata": {
        "id": "jg74XKVyEJc4"
      },
      "id": "jg74XKVyEJc4"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "multiple-tender",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "multiple-tender",
        "outputId": "4de2cdd6-fabd-4d19-a607-7c148b382c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+---------------+\n",
            "|id |raw                             |filtered       |\n",
            "+---+--------------------------------+---------------+\n",
            "|0  |[I, LIKE, the, UNIT, ?]         |[LIKE, UNIT, ?]|\n",
            "|1  |[tHANKS, AGAIN, for, your, help]|[tHANKS, help] |\n",
            "+---+--------------------------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"I\", \"LIKE\", \"the\", \"UNIT\", \"?\"]),\n",
        "    (1, [\"tHANKS\", \"AGAIN\", \"for\", \"your\", \"help\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N Gram Tokenization"
      ],
      "metadata": {
        "id": "a9UIfQCXEE9o"
      },
      "id": "a9UIfQCXEE9o"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "figured-render",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "figured-render",
        "outputId": "ba3b1543-c629-4559-d285-6d1fdfa6778d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------+\n",
            "|ngrams                                                     |\n",
            "+-----------------------------------------------------------+\n",
            "|[hi this, this is, is from, from sit742]                   |\n",
            "|[i think, think sit742, sit742 is, is a, a good, good unit]|\n",
            "|[spark is, is not, not hard, hard to, to learn]            |\n",
            "+-----------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi this is from SIT742\"),\n",
        "    (1, \"I THINK sit742 IS a good unit\"),\n",
        "    (2, \"Spark is not hard to learn\")\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "wordDataFrame = tokenizer.transform(sentenceDataFrame)\n",
        "\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer Representation"
      ],
      "metadata": {
        "id": "XcUpFId3JPZr"
      },
      "id": "XcUpFId3JPZr"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Create initial DataFrame\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"Hi this is from SIT742\".split(\" \")),\n",
        "    (1, \"I THINK sit742 IS a good unit\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# Define a UDF to lowercase all words\n",
        "lowercase_udf = udf(lambda words: [w.lower() for w in words], ArrayType(StringType()))\n",
        "\n",
        "# Apply the UDF\n",
        "df_lower = df.withColumn(\"words_lower\", lowercase_udf(col(\"words\")))\n",
        "\n",
        "# Fit a CountVectorizerModel from the lowercased corpus\n",
        "cv = CountVectorizer(inputCol=\"words_lower\", outputCol=\"features\", minDF=1.0)\n",
        "model = cv.fit(df_lower)\n",
        "result = model.transform(df_lower)\n",
        "\n",
        "# Show results\n",
        "result.select(\"id\", \"words_lower\", \"features\").show(truncate=False)\n",
        "\n",
        "print(\"Vocabulary learned by CountVectorizer:\")\n",
        "print(model.vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ietrDAHg5d",
        "outputId": "eda61dd9-48eb-44ba-f423-57cbdb907805"
      },
      "id": "t0ietrDAHg5d",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------+--------------------------------------------------+\n",
            "|id |words_lower                          |features                                          |\n",
            "+---+-------------------------------------+--------------------------------------------------+\n",
            "|0  |[hi, this, is, from, sit742]         |(10,[0,1,2,3,8],[1.0,1.0,1.0,1.0,1.0])            |\n",
            "|1  |[i, think, sit742, is, a, good, unit]|(10,[0,1,4,5,6,7,9],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "+---+-------------------------------------+--------------------------------------------------+\n",
            "\n",
            "Vocabulary learned by CountVectorizer:\n",
            "['is', 'sit742', 'this', 'hi', 'think', 'a', 'unit', 'i', 'from', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "annoying-browser",
      "metadata": {
        "id": "annoying-browser"
      },
      "source": [
        "# Part B Adcanced Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binarizer on the Continuous Feature"
      ],
      "metadata": {
        "id": "XV20R1S6D2je"
      },
      "id": "XV20R1S6D2je"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "physical-southeast",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "physical-southeast",
        "outputId": "52de9c36-209a-4263-a78b-a92584e75dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binarizer output with Threshold = 9.300000\n",
            "+---+-------+-----------------+\n",
            "| id|feature|binarized_feature|\n",
            "+---+-------+-----------------+\n",
            "|  0|    3.1|              0.0|\n",
            "|  1|    9.3|              0.0|\n",
            "|  2|   27.9|              1.0|\n",
            "+---+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Binarizer\n",
        "\n",
        "continuousDataFrame = spark.createDataFrame([\n",
        "    (0, 3.1),\n",
        "    (1, 9.3),\n",
        "    (2, 27.9)\n",
        "], [\"id\", \"feature\"])\n",
        "\n",
        "binarizer = Binarizer(threshold=9.3, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
        "\n",
        "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
        "\n",
        "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
        "binarizedDataFrame.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA in Spark"
      ],
      "metadata": {
        "id": "WQov7XmKD12u"
      },
      "id": "WQov7XmKD12u"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "adequate-statistics",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adequate-statistics",
        "outputId": "0d165153-03cc-4930-9617-71b5c18c6de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------+\n",
            "|pcaFeatures                                                |\n",
            "+-----------------------------------------------------------+\n",
            "|[4.201827782176354,-9.018506335225522,1.0458592540317526]  |\n",
            "|[-5.324253611770112,-7.0810406514243995,1.0458592540317553]|\n",
            "|[-8.199318861829124,-9.862047628744506,1.0458592540317508] |\n",
            "+-----------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "data = [(Vectors.sparse(5, [(1, 3.5), (2, 9.5)]),),\n",
        "        (Vectors.dense([1.0, 2.0, 4.0, 5.0, 6.0]),),\n",
        "        (Vectors.dense([2.0, 3.0, 5.0, 7.0, 9.0]),)]\n",
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(df)\n",
        "\n",
        "result = model.transform(df).select(\"pcaFeatures\")\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Representation For Continuous Data"
      ],
      "metadata": {
        "id": "GCZ4sTzhEQCO"
      },
      "id": "GCZ4sTzhEQCO"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "worldwide-miracle",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "worldwide-miracle",
        "outputId": "49a73a98-9d56-4b16-fec4-19732b63d92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------------------------------------------------------------------------------------------------------------------+\n",
            "|features |polyFeatures                                                                                                          |\n",
            "+---------+----------------------------------------------------------------------------------------------------------------------+\n",
            "|[3.0,3.0]|[3.0,9.0,27.0,81.0,243.0,3.0,9.0,27.0,81.0,243.0,9.0,27.0,81.0,243.0,27.0,81.0,243.0,81.0,243.0,243.0]                |\n",
            "|[9.0,3.0]|[9.0,81.0,729.0,6561.0,59049.0,3.0,27.0,243.0,2187.0,19683.0,9.0,81.0,729.0,6561.0,27.0,243.0,2187.0,81.0,729.0,243.0]|\n",
            "+---------+----------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import PolynomialExpansion\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (Vectors.dense([3.0, 3.0]),),\n",
        "    (Vectors.dense([9.0, 3.0]),)\n",
        "], [\"features\"])\n",
        "\n",
        "polyExpansion = PolynomialExpansion(degree=5, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
        "polyDF = polyExpansion.transform(df)\n",
        "\n",
        "polyDF.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min Max Scaler in Spark"
      ],
      "metadata": {
        "id": "hyhqpi9vEj3P"
      },
      "id": "hyhqpi9vEj3P"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "drawn-review",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drawn-review",
        "outputId": "a3d4333a-1c15-42e6-9263-873bc67e4c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|            features|      scaledFeatures|\n",
            "+--------------------+--------------------+\n",
            "|[1.0,2.1,-18.0,10...|[0.0,1.0020040080...|\n",
            "|[2.0,2.0,-22.0,20...|[0.33333333333333...|\n",
            "|[4.0,1000.0,18.0,...|[1.0,0.9999999999...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "dataFrame = spark.createDataFrame([\n",
        "    (0, Vectors.dense([1.0, 2.1, -18.0,100]),),\n",
        "    (1, Vectors.dense([2.0, 2.0, -22.0,200]),),\n",
        "    (2, Vectors.dense([4.0, 1000.0, 18.0,0]),)\n",
        "], [\"id\", \"features\"])\n",
        "\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# Compute summary statistics and generate MinMaxScaler\n",
        "scalerModel = scaler.fit(dataFrame)\n",
        "\n",
        "scaledData = scalerModel.transform(dataFrame)\n",
        "\n",
        "scaledData.select(\"features\", \"scaledFeatures\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discretization in Spark"
      ],
      "metadata": {
        "id": "sbffh8JaFCK0"
      },
      "id": "sbffh8JaFCK0"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "champion-lyric",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "champion-lyric",
        "outputId": "e556e16e-ac70-40b0-b550-e37fe615081f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bucketizer output with 4 buckets\n",
            "+--------+----------------+\n",
            "|features|bucketedFeatures|\n",
            "+--------+----------------+\n",
            "|  -221.9|             0.0|\n",
            "|    -0.5|             1.0|\n",
            "|    -0.1|             1.0|\n",
            "|     0.1|             2.0|\n",
            "|     3.2|             3.0|\n",
            "|  1999.9|             3.0|\n",
            "+--------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
        "\n",
        "data = [(-221.9,), (-0.5,), (-0.1,), (0.1,), (3.2,), (1999.9,)]\n",
        "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
        "\n",
        "# Transform original data into its bucket index.\n",
        "bucketedData = bucketizer.transform(dataFrame)\n",
        "\n",
        "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
        "bucketedData.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation in Spark"
      ],
      "metadata": {
        "id": "FhVpkCOmFROP"
      },
      "id": "FhVpkCOmFROP"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "exempt-standard",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exempt-standard",
        "outputId": "9eb55313-1de7-428f-e746-2739fe3ce8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-----+-----+\n",
            "|  a|  b|out_a|out_b|\n",
            "+---+---+-----+-----+\n",
            "|2.0|NaN|  2.0|  6.0|\n",
            "|3.0|NaN|  3.0|  6.0|\n",
            "|NaN|7.0|  3.5|  7.0|\n",
            "|4.0|4.0|  4.0|  4.0|\n",
            "|5.0|7.0|  5.0|  7.0|\n",
            "+---+---+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (2.0, float(\"nan\")),\n",
        "    (3.0, float(\"nan\")),\n",
        "    (float(\"nan\"), 7.0),\n",
        "    (4.0, 4.0),\n",
        "    (5.0, 7.0)\n",
        "], [\"a\", \"b\"])\n",
        "\n",
        "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
        "model = imputer.fit(df)\n",
        "\n",
        "model.transform(df).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now your Turn, could you import a text data and process them with above learned steps?\n",
        "\n",
        "\n",
        "1.   Do you need to consider the N gram tokenization? how it is different with normal Tokenization?\n",
        "2.   How you will conduct the processing if text data is combined with continuous data?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "75MfKKUOFkLU"
      },
      "id": "75MfKKUOFkLU"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "solar-copyright",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "solar-copyright",
        "outputId": "da2c2332-b919-450b-f17b-173497dbccdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------+-----+----------+---------------------------------------+\n",
            "|text                           |label|prediction|probability                            |\n",
            "+-------------------------------+-----+----------+---------------------------------------+\n",
            "|I think the course is difficult|0    |0.0       |[0.5000000000194692,0.4999999999805308]|\n",
            "|What a great subject           |1    |0.0       |[0.5000000000194692,0.4999999999805308]|\n",
            "+-------------------------------+-----+----------+---------------------------------------+\n",
            "\n",
            "Test AUC = 0.500\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"TextLogisticRegression\").getOrCreate()\n",
        "\n",
        "# Sample labeled text data\n",
        "data = [\n",
        "    (0, \"I hate this unit, it's too hard\", 0),\n",
        "    (1, \"This is an amazing class!\", 1),\n",
        "    (2, \"I think the course is difficult\", 0),\n",
        "    (3, \"What a great subject\", 1),\n",
        "    (4, \"This unit is not good\", 0),\n",
        "    (5, \"Excellent content and teaching\", 1)\n",
        "]\n",
        "columns = [\"id\", \"text\", \"label\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Text processing stages\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Build pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, idf, lr])\n",
        "\n",
        "# Train/test split\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Fit model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Evaluate model\n",
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"text\", \"label\", \"prediction\", \"probability\").show(truncate=False)\n",
        "\n",
        "# Compute accuracy\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test AUC = {accuracy:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}