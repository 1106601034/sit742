{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# SIT742: Modern Data Science \n**(Week 03: Data Wrangling)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n\nPrepared by **SIT742 Teaching Team**\n\n---\n\n# Session 3B - Parsing CSV & JSON Files \n\n## Table of Content\n\n* Part 1. Parsing CSV file\n* Part 2. Parsing JSON files\n* Part 3. Summary\n\n\n\n---\n\nDue to advances in technologies for data storage, data from various sources is always stored in different formats\nand file types. Some data formats store data in a way that can be easily handled by a machine, such as CSV, JSON, and XML.\nThose formats are usually referred to as machine-readable formats.\nIn contrast, some other data formats or file types store data in a way meant to be read by a human \nusing front-end desktop tools.\nThose formats or file types are often referred to as hard-to-parse formats.\nWe will use a series of examples to demonstrate how to extract data stored in \nboth machine-readable and hard-to-parse formats,\nand then store the extracted data in formats that can be easily adopted by the downstream data wranngling tasks.\nThis chapter will cover how to read the common machine-readable formats:\n* **CSV**: Comma Separated Values\n* **JSON**: JavaScript Object Notation\n\nIn most cases, the two formats togeather with XML are the best available resource while you are scraping data from\nthe web or requesting data directly from an organization or agency. \nThey are more easily used and ingested by programming languages, like Python.\nOur suggestion is that you should try your best to get data in these formats, before you start looking\ninto other formats that might be hard to parse, like PDFs.\n\nThere are many ways of reading and storing data in those formats, \nwhich depends on the programming language you use.\nHere we are going to focus on Python.\nSearching the Internet, you will find there are a lot of online tutorials on handling data stored in different\ndata formats with Python.\nWe suggest the following:\n* \"*Data Loading, Storage, and File Formats*\", Chapter 6 of \"**Python for Data Analysis**\": This chapter covers reading files in a variety of formats, loading data from databases and interacting with Internet via APIs. Please read pages 155-166, and download and run the Python scripts from [the author's github site](https://github.com/pydata/pydata-book). \ud83d\udcd6\n\nThe dataset used in this chapter was downloaded from\n[data.gov.au](https://data.melbourne.vic.gov.au/Transport-Movement/Melbourne-bike-share/tdvh-n9dv). \nIt is available in the following formats: CSV, JSON, XML, RDF, etc.\nThe first two formats are used, i.e., the following two files\n* Melbourne_bike_share.csv\n* Melbourne_bike_share.json\n\nIn the following sections, you will learn how to scrape data from the two \nexample files, and store the extracted data into Pandas DataFrame. \n\n### Example scenario\nAssume that you are going to analyze and predict bicycle hubway station status to answer the following questions:\n* What do usage patterns look like with respect to specific stations and how that translates to imbalances in the system?\n* Can we integrate these explanatory variables and these usage patterns into a predictive algorithm that would predict empty and full stations in the near future?\n* What form should that algorithm take?\n* How do environmental variables affect the future state of Hubway stations?\n\nSee <a href=\"http://cs109hubway.github.io/classp/\"><font color=\"red\">Predicting Hubway Stations status in Boston</font></a> for more discussion.\n\nThe first step we have to do is to acquire the hub station data and as well as weather data. Here, for demonstration purpose, we use the Melbourne bike share data published by the government. The files have been downloaded and come along with this notebook.\n\n* * *", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 1. Parsing CSV file\n\nA CSV is a Comma Separated Values file, which allows data to be saved in a tabular format.\nEach row of the file is a data record; each column is a field (or an attribute).\nEach data record consists of one or more fields, separated by commas.\nAs one of the most popular file formats,\nit is supported by any spreadsheet programs, such as \nMicrosoft Excel, Open Office Calc, and Google Spreadsheets,\nBecause of its simplicity,\nit differs from other spreadsheet file types, such as Excel, in that one can only store a single sheet in a file. \nIt cannot be used to store cell, columns or row styling, figures and formulas.\nTo make our CSV file, i.e., Melbourne_bike_share.csv, easier to view here, \na sample of the data with trimmed down records is shown below.\nYou should see something similar to this when you open the excel file in your text editor,\n\n![CSV](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/csv1.png \"CSV File\")\n\n\nNote that tabs can also be used to separate values of different fields.\nThis type of files is usually called TSV, Tab Separated Values. \nSometimes TSVs get classified as CSVs.\nThe only difference between CSVs and TSVs is the delimiter.\nEssentially, the two types of files will act the same in Python and most of the other\nprogramming languages. \nIt is worth mentioning that they often take the form of a text file containing information \nseparated by commas.\nThis section will show you how to use Pandas \n[read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function to\nload our CSV file, and how to tidy the loaded data a bit.\nBefore we start importing our CSV file, it might be good for you to read [Pandas tutorial\non reading CSV files](http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table) \ud83d\udcd6.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We can have different ways to inspect your data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install wget", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/Melbourne_bike_share.csv'\n\nDataSet = wget.download(link_to_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!ls", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "with open(\"Melbourne_bike_share.csv\", 'r') as f:\n    for line in f.readlines()[:10]:\n        print (line)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "with open(\"Melbourne_bike_share.csv\", 'r') as f:\n    for line in f.readlines()[-10:]:\n        print (line)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### 1.1. Importing CSV data\nImporting CSV files with Pandas <a href='http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html'><font color = \"blue\">read_csv()</font></a> function and converting the data into a form Python can understand \nis simple. \nIt only takes a couple of lines of code.\nThe imported data will be stored in Pandas DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import pandas as pd\ncsvdf = pd.read_csv(\"Melbourne_bike_share.csv\")\ntype(csvdf)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Or you can use the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html\"><font color='blue'>read_table()</font></a> function", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf_1 = pd.read_table(\"Melbourne_bike_share.csv\", sep=\",\")\ntype(csvdf_1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now, the data should be loaded into Python. \nLet's have a look at the first 5 records in the dataset.\nThere are a coupe of ways to retrieve these records.\nFor example, you can use \n* <font color='blue'>csvdf.head(n = 5)</font>: It will return first `n` rows in a DataFrame, n = 5 by default.\n* <font color='blue'>csvdf[:5]</font>: It uses the slicing method to retrieve the first 5 rows\n\nRefer to \"[Indexing and Selecting Data](http://pandas.pydata.org/pandas-docs/stable/indexing.html)\"\nfor how to slice, dice, and generally get and set subsets of pandas objects.\nHere, we use the `head` function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf.head()\n#csvdf.loc[:4]\n#csvdf[:5]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "csvdf.tail()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Currently, the row indices are integers automatically generated by Pandas.\nSuppose you want to set IDs as row indices and delete the ID column.\nResetting the row indices can be easily done with the following DataFrame function\n```python\n    DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\n```\nSee its [API webpage](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) \nfor the detailed usage.\nThe keys are going to be the IDs in the first column. \nBy setting `inplace = True`, the corresponding change is done inplace and won't return a new DataFrame object.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#len(csvdf.ID.unique())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "csvdf.set_index(csvdf.ID, inplace = True)\ncsvdf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "To remove the ID column that is now redundant, you use DataFrame `drop` function and set `inplace = True`\n```python\n    DataFrame.drop(labels, axis=0, level=None, inplace=False, errors='raise')\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf.drop('ID', 1, inplace = True)\ncsvdf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Instead of using the above method of setting row indices to IDs, you can specify which column to \nbe used as row indices while reading the CSV file. See the API reference page for\n[pandas.read_csv](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html).\nTo do so, you can use the <font color='blue'>index_col</font> argument of <font color='blue'>read_csv()</font>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf = pd.read_csv(\"Melbourne_bike_share.csv\", index_col = \"ID\")\ncsvdf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Similarly, with the <font color='blue'>read_table()</font> function, you can also set the value of <font color='blue'> index_col</font> to \"ID\".", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1.2. Manipulating the Data\n\nSo far, you have learned a little bit about the Melbourne_bike_share data.\nLet's further process the data by splitting the coordinates into latitude and longitude.\nFirst figure out what type of data we're dealing with, i.e., the data type of the \"Coordinates\" column.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "type(csvdf['Coordinates']) \n# type(csvdf.Coordinates)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The data type of this column is Pandas Series, i.e., \na one-dimensional labeled array capable of holding any data type.\nNext, in order to split the coordinates, you should know the data type of those coordinates. Are they strings?\nLet's check them by printing the first element in the Series and its type.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (csvdf['Coordinates'].iloc[0])\ntype(csvdf['Coordinates'].iloc[0]) ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Those coordinates are indeed strings. Thus, to extract both latitude and longitude, you \ncan either use regular expressions introduced in the previous chapter or common string operations.\n\nTo use regular expressions, the key is figuring out the patterns of characters. Then\naccording to those patterns, you formulate your regular expressions.\nLooking at the first couple of coordinates in the Series object, i.e.:\n```\n    (-37.814022, 144.939521)\n    (-37.817523, 144.967814)\n    (-37.84782, 144.948196)\n```\nYou will find that latitudes are always negative real values, and longitudes are positive real values.\nThat is because Australia lies between latitudes 9\u00b0 and 44\u00b0S, and longitudes 112\u00b0 and 154\u00b0E.\nThe regular expression is\n```\n    r\"-?\\d+\\.?\\d*\"\n```\n\n![RegEx](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/regex1.jpg \"RegEx\")\n\nIt contains four parts\n* \"-?\": optionally matches a single '-'.\n* \"\\d+\": matches one or more digits.\n* \"\\\\.?\": optionally matches a single dot.\n* \"\\d*\": matches zero or more digits.\n\nThe following code extracts all real values matching this regular expression.\nThe <font color=\"blue\">re.findall()</font> returns all matched values in a Python list.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import re\nstr1 = csvdf['Coordinates'].iloc[0] # csvdf.Coordinates\nre.findall(r\"-?\\d+\\.?\\d*\", str1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Using common string operations might be simpler than using regular expressions. \n<font color=\"blue\">str.split()</font> is the function used here to extract both latitudes and longitudes.\nHowever, you should choose a proper delimiter to split a string.\nFirst, split the string by ',':", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "s = csvdf['Coordinates'].iloc[1].split(', ') # assuming they're all '(x, y)'\nprint ('lat = ', s[0], ' long = ', s[1])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The printout shows that the latitude contains '(', and the longitude contains ')'.\nYou should consider removing both the left and the right parentheses. \nOf course, the `split` function can be used again. \nNote that the goal here is to remove the leading and trailing parentheses.\nPython string class provides two functions to do the two operations,\nwhich are:\n* <font color=\"blue\">string.lstrip()</font>: returns a copy of the string with leading characters removed\n* <font color=\"blue\">string.rstrip()</font>: returns a copy of the string with trailing characters removed.\n\nLet's try the two functions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (s[0].lstrip('('))\nprint (s[1].rstrip(')'))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The latitude and longitude in the first coordinate have been successfully extracted.\nNext, we are going to apply the extracting process to every coordinate in the DataFrame.\nThere are multiple ways of doing that. \nThe most straightforward way is to write a FOR loop to iterate over all the coordinates,\nand apply the above scripts to each individual coordinate. \nTwo Pandas Series can be then used to store latitudes and longitudes.\nHowever, we are going to show you how to use some advanced Python programming functionality.\n\nPandas Series class implements an [`apply()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) method that applies a given function\nto all values in a Series object, and returns a new one.\nPlease note that this function can only works on single values. \nTo apply <font color=\"blue\">str.split()</font> to every coordinate and\nget latitudes and longitudes, you can use the following two lines of code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf['lat'] = csvdf['Coordinates'].apply(lambda x: x.split(', ')[0])\ncsvdf['lon'] = csvdf['Coordinates'].apply(lambda x: x.split(', ')[1])\ncsvdf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The first line extracts all the latitudes and store them in a column in our DataFrame.\nThe second line extracts all the longitudes.\nYou might wonder what \"lambda\" is in the code. \nIt is a Python keyword used to construct small anonymous functions at runtime. (See [Section 4.7.5. Lambda Expressions](https://docs.python.org/2/tutorial/controlflow.html) \ud83d\udcd6 )\nYou can use a similar approach to remove the heading and trailing parentheses.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf['lat'] = csvdf['lat'].apply(lambda x: x.lstrip('('))\ncsvdf['lon'] = csvdf['lon'].apply(lambda x: x.rstrip(')'))\ncsvdf.drop('Coordinates', 1, inplace = True)\ncsvdf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "So far, we have split the \"Coordinates\" column into two columns, i.e., \"lat\" and 'lon' in the DataFrame,\nand dumped the \"Coordinates\" column.\nThe last step is to infer better type for object columns. \nAll the numerical values and dates are encoded as strings in the current DataFrame.\nWe would like to convert those values to types that they are supposed to have.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf = csvdf.convert_objects(convert_numeric = True) \ncsvdf.dtypes", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "However, dates are still strings, which means the `convert_object` function cannot convert data strings to datatime\nobject.\nHere you need to force them to be converted to datatime object with [`pd.to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "csvdf['UploadDate'] = pd.to_datetime(csvdf['UploadDate'])\nprint (csvdf.dtypes)\ncsvdf", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Finally, you have loaded the given CSV file into Python with Pandas. \nYou have also tidied the data a bit by getting latitudes and longitudes out\nfrom the strings.\n\nBesides `read_csv`, there are other parsing functions in pandas for \nreading tabular data as a DataFrame object. They include\n* [`read_table`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html): Reads general delimited file into DataFrame. The default delimiter is '\\t'.\n* [`read_fwf`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_fwf.html): Reads a table of fixed-width formatted lines into DataFrame.\n* [`read_clipboard`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_clipboard.html): Reads text from clipboard and passes to read_table. See read_table for the full argument list.\n* * *", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 2. Parsing JSON files\n\nJSON (JavaScript Object Notation) is one of the most commonly used formats \nfor transferring data between web services and other applications via HTTP requests.\nNowadays, many sites have JSON-enabled APIs and \nJSON is quickly becoming the encoding protocol of choice.\nAs a light weighted data-interchange format inspired by JavaScript, \nit is clean, easy to read, and easy to parse.\nHere is a simple example adapted from [Wikipedia page on JSON](https://en.wikipedia.org/wiki/JSON)\n```\n[\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Smith\",\n  \"age\": 25,\n  \"address\": {\n    \"streetAddress\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postalCode\": \"10021\"\n   }\n}\n]\n\n```\n\nFrom the above example, you will see that each data record looks like a [Python dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries). \nA JSON file usually contains a list of dictionaries, which is defined by '[' and ']'.\nIn each of those dictionaries,\nthere is a key-value pair for each row and the key and value are separated by a colon.\nDifferent key-value pairs are separated by commas.\nNote that a value can also be a dictionary, see \"address\" in the example.\nThe basic types are object, array, value, string and number.\nIf you would like to know more about JSON, please refer to \n* [Introducing to JSON](http://www.json.org/): the JSON org website gives a very good diagrammatic explanation \nof JSON \ud83d\udcd6.\n* [Introduction to JSON](https://www.youtube.com/watch?v=WWa0cg_xMC8): a 15-minutes Youtube video on JSON, recommended for visual learners.\n\n(Of course, you can also go and find your own materials on JSON by searching the Internet.)\n\nIn the rest of this section, we will start from an simple example, walking through steps of acquiring JSON Data from Google Maps Elevation API and normalizing those data into a flat table. Then, we revisit the dataset mentioned in the previous section (except that it is now in JSON format), parsing the data and store them in a Pandas DataFrame object.\nBefore we start, it might be good for you to view one of the following tutorials on parsing JSON files:\n* [Working with JSON data](http://wwwlyndacom.ezproxy.lib.monash.edu.au/Python-tutorials/Working-JSON-data/122467/142575-4.html): A Lynda tutorial on parsing JSON data. You need a Monash account to access this website.\n[here](http://resources.lib.monash.edu.au/eresources/lynda-guide.pdf) is the lynda settup guide.\n* A [Youtube video](https://www.youtube.com/watch?v=9Xt2e9x4xwQ ) on extracting data from JSON files (**optional**).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2.1 Acquiring JSON Data From The Internet\nThis section will start with showing you how to acquire a small chunk of JSON data\nfrom Internet via HTTP requests and load it into Python with `json` library. \nThe example we used is inspired by the question asked in [Stack Overflow](http://stackoverflow.com/questions/21104592/json-to-pandas-dataframe).\nIn the example, the goal is to extract elevation data from a \n[Google Maps Elevation API](https://developers.google.com/maps/web-services/overview) along\na path specified by latitude and longitude, and convert the JSON data\ninto a Pandas DataFrame object, which could look similar to (but the actual values might vary!)\n\n||elevation|location.lat|location.lng|resolution|\n|------|------|------|------|------|\n|0|243.346268|42.974049|-81.205203|19.087904|\n|1|244.131866|42.974298|-81.195755|19.087904|\n\n\nThe first step is to make a HTTP request to get the data from the Google Maps API.\nHere we are going to use [`urllib2`](https://docs.python.org/2/library/urllib2.html) library.\nIt defines a set of functions and classes that help in opening URLs.\n\nIn order to run the following code, please following the instruction on https://developers.google.com/maps/documentation/elevation/start\nto request a API key.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "locations = \"42.974049,-81.205203|42.974298,-81.195755\"\ntry:\n    from urllib2 import Request, urlopen # for python 2\nexcept ImportError:\n    from urllib.request import urlopen, Request # for python 3\n\napi_key = \"YOUR API-KEY\" #use your own API key her\nrequest = Request(\"https://maps.googleapis.com/maps/api/elevation/json?locations=\"+locations+\"&key=\"+api_key)\n\nresponse = urlopen(request)\nelevations = response.read()\n#elevations.splitlines()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### If you don't have the API, used the pre-dumped json data by enabling the following code", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/elevations.json'\n\nDataSet = wget.download(link_to_data)\n\n!ls", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import json\nwith open(\"elevations.json\", \"r\") as f:\n    elevations=json.load(f)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# elevations = elevations.decode('UTF-8')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "In the above code, we have:\n1. Imports Request class and the <font color=\"blue\">urlopen() </font> function from `urllibs` module.\n2. Defines a path with the coordinates of the start and end points\n3. Creates a URL Request object. Note that you can change the output format by replacing '/json' with '/xml'.\n4. Opens the URL, and returns a file-like object.\n5. Reads data returned from the HTTP request.\n\nThe returned data is actually stored in a string. \nYou can check it out using Python's built-in function `type`, \n```python\n    type(elevations)\n```\nWhat does the data look like?\nIn stead of printing the data in one single string, one can use\n```python\n    elevations.splitlines()\n```\nto print the data as a list of lines in the string, breaking\nat line boundaries, i.e., '\\n'. \nThe printout you get should look like\n```\n['{',\n '   \"results\" : [',\n '      {',\n '         \"elevation\" : 243.3462677001953,',\n '         \"location\" : {',\n '            \"lat\" : 42.974049,',\n '            \"lng\" : -81.205203',\n '         },',\n '         \"resolution\" : 19.08790397644043',\n '      },',\n '      {',\n '         \"elevation\" : 244.1318664550781,',\n '         \"location\" : {',\n '            \"lat\" : 42.974298,',\n '            \"lng\" : -81.19575500000001',\n '         },',\n '         \"resolution\" : 19.08790397644043',\n '      }',\n '   ],',\n '   \"status\" : \"OK\"',\n '}']\n```\nIt is easy to dump the data into a JSON file, which just takes three lines of code:\n```python\n    import json\n    with open(\"data/elevations.json\", \"w\") as outfile:\n         json.dump(elevations, outfile)\n```\n\nTo read the acquired JSON data, you can use the `json` module as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import json\ndata = json.loads(elevations)\nprint (type(data))\ndata", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It loads the data into a Python dictionary.\nThe data we want is stored in the first entry.\nThe value of this entry is a list of two dictionaries, each of which corresponds to a record.\nsee [JSON encoder and decoder](https://docs.python.org/2/library/json.html) for more on reading\nJSON files.\n\nAs mentioned earlier in this section, \nwe will convert the JSON data into Pandas DataFrame.\nTherefore, Pandas functions on reading JSON are to be used.\nIf you would like to know about those functions, you can read Pandas tutorial on [Reading JSON](http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader) (**optional**).\nLet's first try the <font color=\"blue\">read_json()</font> function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Unfortunately, the DataFrame returned by `read_json` is not the one we want.\nYou might wonder why the `read_json` function did not return the DataFrame we want.\nThere is a straight forward answer.\nLet's try to build a DataFrame from `data` returned by \n```\n    data = json.loads(elevations)\n```\nWhat do you get?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import pandas as pd\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pd.DataFrame(data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You have got a DataFrame that is exactly the same as the one returned by `read_json`.\nThis is due to Pandas' way of constructing a DataFrame from a dictionary. \nSee [Intro to Data Structures](http://pandas.pydata.org/pandas-docs/stable/dsintro.html)\nfor constructing a DataFrame from a dictionary\nand \"Object Creation\" in [10 Mintues to Pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html) \ud83d\udcd6.\nIt is not hard to figure out that dictionary keys \nare used as column \nlabels, and values of whatever data types are put as column values.\n\nWhat we want is to flatten out JSON object into a flat table.\nFortunately, Pandas provides a JSON normalization function [(<font color=\"blue\">json_normalize()</font>)](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html)\nthat takes a dict or list of dicts and normalize semi-structured data into a flat table. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pandas.io.json import json_normalize\njson_normalize(data['results'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Eventually, the <font color=\"blue\">json_normalize()</font> function returns the DataFrame we want.\nHowever flattening objects with embedded arrays/lists is not as trivial.\nSee [Flattening JSON objects in Python](https://gist.github.com/amirziai/2808d06f59a38138fa2d)\nfor more information.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2.2. Parsing the \"Melbourne_bike_share.json\"  File\nNow that you have learned how to use `json` module and Pandas together to parse a simple JSON file.\nIn this section we will walk you through the process of extracting bike hub station statistical data from \"Melbourne_bike_share.json\". Then produce the same DataFrame as the one in Section 1.\n\nRemember that the first step is always to glance through the JSON file with your favorite editor.\nBelow is the first 20 lines from our JSON file.\n\n![JSON](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/json20.png \"JSON File\")\n\nThis JSON file is much more complex that the one used in the previous section\nIt might take a bit of time to figure out that this file is a dictionary of \ntwo large dictionaries, one with key \"meta\", and another with \"data\".\nThe \"meta\" dictionary contains all the meta information, including column names.\nThe \"data\" dictionary actually contains the data we want.\nIn the following subsection, we will show you how to extract records from the \"data\"\ndictionary, while leaving the task of extracting column labels from the \"meta\" dictionary as an exercise.\nSimilarly, our JSON data can be read into Python as follows.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/Melbourne_bike_share.json'\n\nDataSet = wget.download(link_to_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import json\nwith open(\"Melbourne_bike_share.json\") as json_file:\n    json_data = json.load(json_file)\nprint (type(json_data))\njson_data['meta']['view']", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The loaded JSON data has been saved in a Python dictionary with two entries, one for \"data\" and another for \"meta\".\nUsing `json_normalize`, you can flatten the \"data\" dictionary into a table and save it in a DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df = json_normalize(json_data,'data')\ndf.head()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We seem to have a lot of extra columns.\nThe data we want starts at column 8.\nTherefore, dump all the irrelevant preceding columns.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "try:\n    df.drop(xrange(8), axis=1, inplace=True)\nexcept:\n    df.drop(range(8), axis=1, inplace=True)\n\ndf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Renaming all the columns with the field names given by the CSV file. \nYou can programmatically extract field names from the \"meta\" dictionary.\nWe will leave it for you to do as an exercise.\nSimilar to parsing CSV file, IDs are unique and can be set to row indices. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.columns = ['id','featurename','terminalname','nbbikes','nbemptydoc','uploaddate','coordinates']\ndf.set_index(df.id, inplace= True)\ndf.drop('id', 1, inplace = True)\ndf.head()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "What's in the last two columns?\n\"uploaddate\" is supposed to have a standard datetime format in the column,\nand coordinates should be pairs of latitude and longitude.\nBoth of them should be real numbers.\nAt the moment, a datetime is encoded as a 64-digit integer (i.e., datetimes in milliseconds since epoch),\nand a coordinate is a Python list as\n```python\n [u'{\"address\":\"\",\"city\":\"\",\"state\":\"\",\"zip\":\"\"}',\n u'-37.814022',\n u'144.939521',\n None,\n False]\n```\nLet's first convert those integers into standard datetime.\nThe following Python code converts \none of these integers into a standard datetime using Python\n[`datatime`](https://docs.python.org/2/library/datetime.html) module:\n```python\n    import datatime\n    date = datetime.datetime.fromtimestamp(df.iloc[0,4])\n    print data\n```\nThe output is \n```\n    2016-01-28 23:45:05\n```\nSimilar to the way of splitting coordinates in Section 2.1, \none can use `pandas.Series.apply` to invoke  `datetime.datetime.fromtimestamp`\non each individual integer in the column. \nPlease try this method by yourself.\n\nInstead, we will show you a pandas specific way of converting \ntimestamp values in milliseconds into standard datetime.\nHere we use Pandas [`to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html)\nfunction.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['uploaddate'] = pd.to_datetime(df['uploaddate'], unit='s')\ndf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Note that the unit argument must be explicitly specified. It can take values on (D,s,ms,us,ns).\nWithout specifying its value, `1453985105`, for example, will be converted to some strange date as\n```\n    Timestamp('1970-01-01 00:00:01.453985105')\n```\nYou can compare the converted dates with those in the DataFrame constructed from our CSV file.\nFor example,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (csvdf.iloc[0,4]) # the csv date\nprint (df.iloc[0,4]) ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The difference is due to that two files were downloaded one after another.\nHowever, the time format is the same.\n\nThe last step is to extract latitudes and longitudes into two columns.\nEach coordinate in the last column of the DataFrame is a Python list.\nThe second and the third entries are latitude and longitude respectively.\nIt is very easy to get the two entries into a list.\nWe will apply the following anonymous function to all the coordinates one after another\n```python\n    lambda col: col[i]\n```\nwhere i = 1 or 2. While i = 1, it returns latitudes; i = 2, it returns longitudes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df['lat'] = df['coordinates'].apply(lambda col: col[1]) # arrrrgh\ndf['lon'] = df['coordinates'].apply(lambda col: col[2])\ndf.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now, dump the \"coordinates\" columns and change data type of each column.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.drop('coordinates', 1, inplace = True)\ndf = df.convert_objects(convert_numeric=True) \ndf", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## 3. Summary\n\nFiles in either CSV or JSON format are the easiest ones to preview, understand and parse. \nIn this chapter\uff0cyou have learned about how to pull data out from files stored in those two formats\nusing Pandas. You should now be familiar with these two formats.\n\n## Exercises\n1. To further parse the Excel file, try the following \n    1. Split the \"Featurename\" into bike hub station's street name, and suburb name, then store them in three columns.\n    2. Extract date and time from the \"UploadDate\" columns, store them in two different columns.\n2.  Section 3.2 has shown you how to extract data from the given JSON file. However, it did not show\nhow to programmatically extract column labels from the meta data. The task here is to extract all\nthe column labels from the metadata using either <font color='blue'>json_normalize()</font> function or the way you prefer.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "name": "2. Parsing CSV & JSON Files.ipynb"
    }, 
    "nbformat": 4
}