{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdDP2so4WLox"
   },
   "source": [
    "# SIT742: Modern Data Science \n",
    "**(Week 05: Text Analysis)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "## Session 5B - Exploring Pre-Processed text and Generating Features\n",
    "\n",
    "### Table of Content\n",
    "\n",
    "* Part 1. Counting Vocabulary by Selecting Tokens of Interest\n",
    "* Part 2. Building Vector Representation \n",
    "* Part 3. Saving Pre-processed Text to a File\n",
    "* Part 4. Extracting Other Features\n",
    "* Part 5. Summary\n",
    "* Part 6. Reading Materials\n",
    "* Part 7. Exercises\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "One of the challenges of text analysis is to convert unstructured and semi-structured text into a structured representation. This must be done prior to carrying out any text analysis tasks. This chapter will show you \n",
    "how to put some of those basic steps discussed in the previous chapter together to generate different vector\n",
    "representations for some given text. You will learn how to compute some basic statistics for text, and how to extract features rather than unigrams.\n",
    "\n",
    "\n",
    "## Part 1. Counting Vocabulary by Selecting Tokens of Interest\n",
    "\n",
    "Two important concepts that should be mentioned first are **type** and **token**.\n",
    "Here are the definitions of the two terms, quoted from \"[tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\",  \n",
    ">a **token** is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing;\n",
    "\n",
    "> a **type** is the class of all tokens containing the same character sequence. \n",
    "\n",
    "A *type* is also a vocabulary entry. In other words, a vocabulary consists of a number of word types.\n",
    "The distinction between a type and its tokens is a distinction that separates a descriptive concept from\n",
    "its particular concrete instances. \n",
    "This is quite similar to the distinction in object-oriented programming between classes and objects.\n",
    "In this section, you are going to learn how to count types in a given corpus by further processing the text.\n",
    "\n",
    "The document collection that we are going to use is a set of Gutenberg books that comes with NLTK.\n",
    "It contains 18 books of 12 authors in total.\n",
    "Although this collection has already been pre-processed (e.g., you can access the text at different levels, like raw text, tokens, and sentences),\n",
    "we would still like to demonstrate how to put some of the basis text preprocessing steps together and process the raw Gutenberg books step by step.\n",
    "First, import the main Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WLSkQVfWLox",
    "outputId": "b8514b4a-4eac-4cae-fc79-7165f4cc9d56"
   },
   "outputs": [],
   "source": [
    "#matplotlib package is used for Data virtualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "#nltk.tokenize.punkt module. \n",
    "#This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. \n",
    "#It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkNQeE9rWLoz"
   },
   "source": [
    "Since the tokenizer works on a per document level, we can parallelize the process of tokenization with Python's multi-processing module. Please refer to its official documentation [here](https://docs.python.org/2/library/multiprocessing.html).\n",
    "In the following code, we wrap tokenization in a Python function, and then\n",
    "create a pool of four worker processes with the Python Pool class.\n",
    "The <font color=\"blue\">Pool.map()</font>, a parallel equivalent of the  built-in  <font color=\"blue\">map()</font> function, takes one iterable argument.\n",
    "The iterable will be split into a number of chunks, each of which will be submitted to a process in the process pool.\n",
    "Each process will apply a callable function to each element in the chunk it has received.\n",
    "Note that you can replace the NLTK tokenizer with the one you implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwpTIoxZWLo0"
   },
   "outputs": [],
   "source": [
    "#Please recall how to define a function using Python programming language\n",
    "def tokenizeRawData(fileid):\n",
    "    \"\"\"\n",
    "        This function tokenizes a raw text document.\n",
    "    \"\"\"\n",
    "    #cover all words to lowercase\n",
    "    raw_article = gutenberg.raw(fileid).lower()\n",
    "    #Tokenize each gutenberg books\n",
    "    #Return a tokenized copy of text, using NLTK’s recommended word tokenizer \n",
    "    #(currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\n",
    "    tokenised_article = nltk.tokenize.word_tokenize(raw_article) \n",
    "    return (fileid, tokenised_article) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtIYDy4cucij",
    "outputId": "802e41c8-d5cb-45ec-ac5b-59f9ea613cee"
   },
   "outputs": [],
   "source": [
    "#The gutenberg can be grouped into 19 books.\n",
    "#This fileids is automatically detect the topic of a document,\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WbHjl51tSwL"
   },
   "outputs": [],
   "source": [
    "#We run the above own defined function tokenizeRawData\n",
    "tokenized_gutenberg =  dict(tokenizeRawData(fileid) for fileid in gutenberg.fileids())\n",
    "tokenized_gutenberg\n",
    "\n",
    "#You can compare the tokenized_gutenberg results with the raw text document from the gutenberg.words('test/14826')[:14]\n",
    "#tokenized_gutenberg will show the tokenized results of the raw text document\n",
    "#In the raw text document, 'U',',''S' are three tokens but after the tokenization, you will find they are one token u.s.-japan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZhdNJFaWLo3"
   },
   "source": [
    "### 1.1. Removing Words with Non-alphabetic Characters\n",
    "The NLTK's built-in  <font color=\"blue\">word_tokenize</font> function tokenizes a string to split off punctuation other than periods.\n",
    "Not only does it return words with alphanumerical characters, but also punctuations. \n",
    "Let's take a look at one Gutenberg book,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jX1RImY6WLo3",
    "outputId": "d0c7df4d-c22d-43e8-acd5-3b55ea1e356c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To print all key and vaule in the tokenized_reruers dictionary\n",
    "for key, value in tokenized_gutenberg.items() :\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9F9E6aAw-fs"
   },
   "outputs": [],
   "source": [
    "#To chose one key from the above dictionary content and show its related values\n",
    "tokenized_gutenberg['austen-emma.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjgiQUe_WLo5"
   },
   "source": [
    "Let's Assume that we are interested in words containing alphabetic characters only \n",
    "and would like to remove all the other tokens\n",
    "that contain digits, punctuation and the other symbols.\n",
    "Removing all the non-alphabetic words from the vocabulary is\n",
    "usually required in some text analysis tasks, such as Topic Modelling that\n",
    "learns the semantic meaning of documents.\n",
    "It can be easily done with the  <font color=\"blue\">isalpha()</font> function.\n",
    " <font color=\"blue\">isalpha()</font>\n",
    "checks whether the string consists of alphabetic characters only or not.\n",
    "This method returns true if all characters in the string are in the alphabet and there \n",
    "is at least one character, false otherwise.\n",
    "If you would like to keep all words with alphanumeric characters, you can use\n",
    " <font color=\"blue\">isalnum()</font>. Refer to Python's [built-in types](https://docs.python.org/2/library/stdtypes.html) for more detail.\n",
    "Indeed, you can construct your tokenizer in a way such that the tokenizer only extracts words with either \n",
    "alphabetic or alphanumerical characters, as we discussed in the previous chapter.\n",
    "We will leave this as a simple exercise for you to do on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gmc5cy-WLo6",
    "outputId": "49366c11-e83b-4b5e-a33f-ebe03f37a60f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in tokenized_gutenberg.items():\n",
    "    #For the v in the each k, we will  only keep alphabetic characters\n",
    "    tokenized_gutenberg[k] = [word for word in v if word.isalpha()]\n",
    "\n",
    "tokenized_gutenberg['austen-emma.txt']\n",
    "#You will find the ',' and numbers which are deleted, only keep the alpha in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fsIT7jA6wAq"
   },
   "outputs": [],
   "source": [
    "for k, v in tokenized_gutenberg.items():\n",
    "    #For the v in the each k, we will  keep alphabetic characters and alphanumberic characters\n",
    "    tokenized_gutenberg[k] = [word for word in v if word.isalnum()]\n",
    "\n",
    "tokenized_gutenberg['austen-emma.txt']\n",
    "#You will find the ',' and numbers which are not deleted comparing with above sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEihhdBaWLo7"
   },
   "source": [
    "Now you should have derived much cleaner text for each Gutenberg book.\n",
    "Let's check how many types we have in the whole corpus and the lexical diversity (i.e., the average number \n",
    "of times a type apprearing in the collection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEIr4PLRWLo7",
    "outputId": "81c0b3ad-8b3e-4639-839e-e7c138de0dc6"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "words = list(chain.from_iterable(tokenized_gutenberg.values()))\n",
    "\n",
    "#Convert the words type from list to set\n",
    "#because the set will automaticlly delete all repeated elements in the list.\n",
    "vocab = set(words)\n",
    "\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "\n",
    "\n",
    "print (\"Total number of tokens: \", len(words), \"\\nVocabulary size (Type): \",len(vocab),\n",
    "\"\\nLexical diversity: \", lexical_diversity, \"\\nThe Lexical diversity means that words occure on averge about\", round(lexical_diversity),\"times each.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZGQ1W_WWLo-"
   },
   "source": [
    "There are about 2.09 million word tokens in the tokenized Gutenberg corpus.\n",
    "The vocabulary size is 40,753, which is still quite large according to our knowledge of this corpus.\n",
    "The lexical diversity tells us that words occur on average about 51 times each.\n",
    "You might think that\n",
    "there could still be words that occur very frequently, such as stopwords,\n",
    "and those that only occur once or twice.\n",
    "For example, if an article \"the\" appears in almost\n",
    "every document in a corpus,\n",
    "it might not help you at all and would only contribute noise.\n",
    "Similarly if a word appears only once in a corpus or only in one document of the corpus,\n",
    "it could carry little useful information for downstream analysis.\n",
    "Therefore, we would better remove those words from the vocabulary, which\n",
    "will benefit the text analysis algorithms in terms of reducing running time and\n",
    "memory requirement, and improving their performance.\n",
    "To do so, we need to further explore the corpus by computing some simple\n",
    "statistics.\n",
    "\n",
    "Note that we introduced two new Python libraries in the code above.\n",
    "They are\n",
    "[`__future__`](https://docs.python.org/3.8/library/__future__.html) \n",
    "and [`itertools`](https://docs.python.org/3.8/library/itertools.html). \n",
    "\n",
    "The first statement in the code makes sure that Python switches to \n",
    "always yielding a real result.\n",
    "Thus if you divide two integer values, you will not get for example. \n",
    "````\n",
    "    1/2 = 0\n",
    "    3/2 = 1\n",
    "````\n",
    "Instead, you will have\n",
    "```\n",
    "    1/2 = 0.5\n",
    "    3/2 = 1.5\n",
    "```\n",
    "The second statement imported a  [chain](https://docs.python.org/3.8/library/itertools.html) iterator from the  <font color=\"blue\">itertools</font> module.\n",
    "We use the iterator to join all the words in all the Gutenberg books together.\n",
    "It works as\n",
    "```python\n",
    "   for wordList in tokenized_gutenberg.values():\n",
    "       for word in wordList:\n",
    "           yield word\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1bzCKsoWLo-"
   },
   "source": [
    "### 1.2. Removing the Most and Less Frequent Words\n",
    "It is quite useful for us to identify the words that are most informative about the sematic \n",
    "meaning of the text regardless of syntax.\n",
    "One common statistics often used in text processing is frequency distribution.\n",
    "It can tell us how frequent a word is in a given corpus in terms of either term frequency or document frequency.\n",
    "Term frequency counts the number of times a word occurs in the whole corpus regardless which document it is in.\n",
    "Frequency distribution based on term frequency tells us how the total number of word tokens are distributed across all the types.\n",
    "NLTK provides a built-in function `FreqDist` to compute this distribution directly from a set of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qt6WWs-sWLo_"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "fd_1 = FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te2Lfh9GWLpB"
   },
   "source": [
    "What are the most frequent words in the corpus?\n",
    "we can use the  <font color=\"blue\">most_common</font> function to print out the most frequent words together with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qcs3io_rWLpC",
    "outputId": "ef3dc481-929c-4568-abb4-c7ba097e19ce"
   },
   "outputs": [],
   "source": [
    "fd_1.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nQpVnNQWLpF"
   },
   "source": [
    "The list above contains the 25 most frequent words.\n",
    "You can see that it is mostly dominated by the little words of the English language which have important grammatical roles.\n",
    "Those words are articles, prepositions, pronouns, auxiliary webs, conjunctions, etc.\n",
    "They are usually referred to as function words in linguistics, which tell us nothing about \n",
    "the meaning of the text.\n",
    "What proportion of the text is taken up with such words?\n",
    "We can generate a cumulative frequency plot for them\n",
    "using  <font color=\"blue\">fd.plot(25, cumulative=True)</font>.\n",
    "If you set  <font color=\"blue\">cumulative</font> to  <font color=\"blue\">False</font>, \n",
    "it will plot the frequencies of these 25 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "uKhAUfidWLpF",
    "outputId": "84ad0dbc-8ce2-4d03-ed4c-96d1c5175d77"
   },
   "outputs": [],
   "source": [
    "fd_1.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuzFoWggWLpH"
   },
   "source": [
    "What are the most frequent words in terms of **Document Frequency**?\n",
    "Here we are going to count how many documents a word appears in, which is referred to as document frequency.\n",
    "Instead of writing nested FOR loops to count the document frequency for each word,\n",
    "we can use  <font color=\"blue\">FreqDist()</font> jointly with  <font color=\"blue\">set()</font> as follows:\n",
    "1. Apply  <font color=\"blue\">set()</font> to each Gutenberg article to generate a set of unique words in the article and save all sets in a list\n",
    "```python\n",
    "    [set(value) for value in tokenized_gutenberg.values()]\n",
    "```\n",
    "2. Similar to what we have done before, we put all the words in a list using  <font color=\"blue\">chain.from_iterable</font> and past\n",
    "it to  <font color=\"blue\">FreqDist</font>.\n",
    "\n",
    "The first step makes sure that each word in an article appears only once, thus the total number of \n",
    "times a word appears in all the sets is equal to the number of documents containing that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqS2tUMTWLpI",
    "outputId": "fdc4183d-297c-4fbf-df17-6b862bfed697"
   },
   "outputs": [],
   "source": [
    "words_2 = list(chain.from_iterable([set(value) for value in tokenized_gutenberg.values()]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "fd_2.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtMZvFlmWLpJ"
   },
   "source": [
    "What you will find is that the majority of the most frequent words according to their document frequecy are still functional words.\n",
    "Therefore, the next step is to remove all the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmVwW4IFWLpK"
   },
   "source": [
    "#### 1.2.1 Ignoring Stopwords\n",
    "\n",
    "We often remove function words from the text completely for most text analysis tasks.\n",
    "Instead of using the built-in stopword list of NLTK, we use a much rich stopword list.\n",
    "\n",
    "Stop Words also named common words. Most Search Engines do not consider extremely common words in order to save disk space or to speed up search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVCL9iT_WLpK",
    "outputId": "d6f2678c-2841-41d5-ad01-b90ee0d7ec91"
   },
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "midXBh4_WLpM"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/stopwords_en.txt'\n",
    "\n",
    "DataSet = wget.download(link_to_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDKOG8WcWLpO",
    "outputId": "20984b24-46d0-40cc-dbb1-a59ddf856354"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HktrUxhkWLpQ",
    "outputId": "5e37d1c9-f75e-4e06-da28-4315e891c470"
   },
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwords[:20]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Rw_8zBsWLpR",
    "outputId": "9adc3069-1f44-47dc-9c0e-30c1480038e6"
   },
   "outputs": [],
   "source": [
    "tokenized_gutenberg_1 = {}\n",
    "for fileid in gutenberg.fileids():\n",
    "    tokenized_gutenberg_1[fileid] = [w for w in tokenized_gutenberg[fileid] if w not in stopwords]\n",
    "tokenized_gutenberg_1\n",
    "print(len(tokenized_gutenberg_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrCGHXQWLpS"
   },
   "source": [
    "The list comprehension \n",
    "```python\n",
    "    [w for w in tokenized_gutenberg[fileid] if w not in stopwords]\n",
    "```\n",
    "says: For each word in each Gutenberg Book, keep the word if the word is not contained in the stopword list.\n",
    "\n",
    "Checking for membership of a value in a list takes time proportional to the list's length in the average and worst cases. \n",
    "It causes the above code to run quite slow as we need to do the check for every word in each Gutenberg Book\n",
    "and the size of the stopword list is large.\n",
    "However, if you have hashable items, which means both the item order and duplicates are disregarded, \n",
    "Python `set` is better choice than `list`, [Why [Click here]?](https://stackoverflow.com/questions/3489071/in-python-when-to-use-a-dictionary-list-or-set) The former runs much faster than the latter in terms of searching\n",
    "a large number of hashable items. Indeed, `set` takes constant time to check the membership.\n",
    "Let's try converting the stopword list into a stopword set, then search to remove all the stopwords.\n",
    "Please also note that if you try to perform iteration, `list` is much better than `set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZdC1YXJWLpT",
    "outputId": "73d48891-6549-41b8-a5d9-0a112b606c85"
   },
   "outputs": [],
   "source": [
    "#Convert the list stopwords to set stopwordsSet\n",
    "stopwordsSet = set(stopwords)\n",
    "\n",
    "#The length of stopwords list\n",
    "print(len(stopwords))\n",
    "\n",
    "#The length of stopwordsSet set\n",
    "print(len(stopwordsSet))\n",
    "\n",
    "#To delete all stopswords in the gutenberg\n",
    "for fileid in gutenberg.fileids():\n",
    "    tokenized_gutenberg[fileid] = [w for w in tokenized_gutenberg[fileid] if w not in tokenized_gutenberg]\n",
    "\n",
    "#The length of processed tokenized_gutenberg\n",
    "print(len(tokenized_gutenberg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyV5VRyfWLpV"
   },
   "source": [
    "In the above stopping process, stopwords have been removed from the vocabulary. You might wonder what those removed words are. It is quite easy to check those words by differentiating the vocabulary before and after stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZOljvS4WLpW",
    "outputId": "90dc8af1-a373-4342-ee75-57667e162dd0"
   },
   "outputs": [],
   "source": [
    "#To combine all tokenized_gutenberg without stopwords in a list using the chain method\n",
    "words_3 = list(chain.from_iterable(tokenized_gutenberg.values()))\n",
    "fd_3 = FreqDist(words_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uptxxrh-WLpX"
   },
   "source": [
    "Beside stopwords, there might some other words that occur quite often as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SHdWCmxHWLpY",
    "outputId": "4f346214-2d51-45fd-a4ba-2fcf27771d69"
   },
   "outputs": [],
   "source": [
    "fd_3.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhtHw76AWLpZ"
   },
   "source": [
    "Before we decide to remove those words from our vocabulary, it might be worth checking what \n",
    "those words mean and the context of those words. Fortunately NLTK provides a `concordance`\n",
    "function in the `nltk.text` module. A concordance view shows us every occurrence of a given \n",
    "word, together with the corresponding context. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwJXseY3WLpZ",
    "outputId": "f158b958-c2cf-4da5-bd09-35d484f4301d"
   },
   "outputs": [],
   "source": [
    "nltk.Text(gutenberg.words()).concordance('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZSWkLnKWLpc",
    "outputId": "3967eb8d-3755-4c6a-844f-83da2776de20"
   },
   "outputs": [],
   "source": [
    "nltk.Text(gutenberg.words()).concordance('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KsFTpUXWLpd"
   },
   "source": [
    "After reviewing those words, you might also want to remove them from the vocabulary. \n",
    "We will leave it as an excersie for you to do on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oumHypz-WLpd"
   },
   "source": [
    "#### 1.2.2 Remove Less Frequent Words\n",
    "\n",
    "If the most common words do not benefit the downstream text analysis tasks, except for contributing noises,\n",
    "how about the words that occur once or twice?\n",
    "Here another interesting statistic to look at is the frequency of the frequencies of word types in a given corpus.\n",
    "We would like to see how many words appear only once, how many words appear twice, how many\n",
    "words appear three times, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "IAbe0bzSWLpe",
    "outputId": "01f4ab8e-ef55-4b1d-b58e-8d54ee9be8ab"
   },
   "outputs": [],
   "source": [
    "#we have defined fd_3 = FreqDist(words_3), it means that ffd will show FreqDist about the processed tokenized_gutenberg\n",
    "ffd = FreqDist(fd_3.values())\n",
    "\n",
    "#Import pylab package for data virtualization\n",
    "from pylab import *\n",
    "\n",
    "#To create a list include 14 element, '0'.\n",
    "y = [0]*14\n",
    "\n",
    "#For top 10 Freq, we will show each of them \n",
    "#For therest Freq, we will catagorize them by 10-50, 51-100, 101-500 and > 500.\n",
    "#You can add a new code cell to run the command 'ffd'. Then, you can understand the below statements block deeply.\n",
    "for k, v in ffd.items():\n",
    "     if k <= 10:\n",
    "        y[k-1] = v\n",
    "     elif k >10 and k <= 50:\n",
    "        y[10] =  y[10] + v\n",
    "     elif k >50 and k <= 100:\n",
    "        y[11] =  y[11] + v\n",
    "     elif k > 100 and k <= 500:\n",
    "        y[12] =  y[12] + v\n",
    "     else:\n",
    "        y[13] =  y[13] + v\n",
    "                \n",
    "#covert a integer list to a string list\n",
    "ytks =list(map(str, range(1, 11))) \n",
    "\n",
    "#append is uedd to add its argument as a single element to the end of a list. \n",
    "ytks.append('10-50')\n",
    "ytks.append('51-100')\n",
    "ytks.append('101-500')\n",
    "ytks.append('>500')\n",
    "\n",
    "# generate integer from 1 to 14        \n",
    "x = range(1, 15)\n",
    "\n",
    "#Center the bars on the y positions\n",
    "barh(x,y, align='center')\n",
    "\n",
    "#x, A list of positions at which ticks should be placed\n",
    "#ytks, A list of explicit labels to place at the given locs.\n",
    "yticks(x, ytks)\n",
    "\n",
    "#To define x axial and y axial lables.\n",
    "xlabel('Frequency of Frequency')\n",
    "ylabel('Word Frequency')\n",
    "\n",
    "#To show the grid line on the diagram\n",
    "grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgGBnZXYWLpg"
   },
   "source": [
    "The horizontal bar chart generated above shows how many word types occur with a certain frequency.\n",
    "Similarly, you can also look at the bar chart based on the document frequency. Try it by yourself!\n",
    "\n",
    "Let's further remove those words that occur only once. \n",
    "To get those words, you can write the code like\n",
    "```python\n",
    "    lessFreqWords = set([k for k, v in fdist.items() if v < 2])\n",
    "```\n",
    "or choose to use `hapaxes()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNIsj1DOWLpg"
   },
   "outputs": [],
   "source": [
    "lessFreqWords = set(fd_3.hapaxes())\n",
    "\n",
    "#You also can use  \"lessFreqWords = set([k for k, v in fdist.items() if v < 2])\" to replace \"lessFreqWords = set(fd_3.hapaxes())\"\n",
    "\n",
    "#You can uncomment below codes for using the mp.pool parallelizing the execution of a function.\n",
    "#pool = mp.Pool(4)\n",
    "#tokenized_gutenberg = dict(pool.map(removeLessFreqWords, gutenberg.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6Ap4JxoWLpi"
   },
   "outputs": [],
   "source": [
    "#To define a function for removing LessFreqWords\n",
    "def removeLessFreqWords(fileid):\n",
    "    return (fileid, [w for w in tokenized_gutenberg[fileid] if w not in lessFreqWords])\n",
    "\n",
    "tokenized_gutenberg = dict(removeLessFreqWords(fileid) for fileid in gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVVtApqdWLpj"
   },
   "source": [
    "Now, you should have a pretty clean set of Gutenberg books, each of which is stored as a list of word tokens.\n",
    "Let's further print out some statistics that summarize this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAHMztFKWLpk",
    "outputId": "90e4fd57-0757-478c-fe3e-9c47abc0b82b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = list(chain.from_iterable(tokenized_gutenberg.values()))\n",
    "vocab = set(words)\n",
    "\n",
    "print (\"Vocabulary size: \",len(vocab))\n",
    "print (\"Total number of tokens: \", len(words))\n",
    "print (\"Lexical diversity: \", lexical_diversity)\n",
    "print (\"Total number of books:\", len(tokenized_gutenberg))\n",
    "\n",
    "lens = [len(value) for value in tokenized_gutenberg.values()]\n",
    "print('The lens list for each article are', lens)\n",
    "\n",
    "#For mean,max, min and std,you can check those concepts on https://www.ucd.ie/t4cms/Mean%20and%20Standard%20Deviation.pdf\n",
    "print (\"Average document length:\", np.mean(lens))\n",
    "print (\"Maximun document length:\", np.max(lens))\n",
    "print (\"Minimun document length:\", np.min(lens))\n",
    "print (\"Standard deviation of document length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfkveI3zWLpm"
   },
   "source": [
    "## Part 2. Building Vector Representation\n",
    "\n",
    "After text pre-processing has been completed, each individual document needs to be transformed into \n",
    "some kind of numeric representation that can be input into most NLP and text mining algorithms.\n",
    "For example, classification algorithms, such as Support Vector Machine, can only take data in a \n",
    "structured and numerical form. They do not accept free languge text.\n",
    "The most popular structured representation of text is the vector-space model, which represents text\n",
    "as a vector where the elements of the vector indicate the occurence of words within the text.\n",
    "The vector-space model makes an implicit assumption that \n",
    "the order of words in a text document are not as\n",
    "important as words themselves, and thus disregarded.\n",
    "This assumpiton is called [**Bag-of-words**](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "Given a set of documents and a pre-defined list of words appearing \n",
    "in those documents (i.e., a vocabulary), you can compute a vector representation for each document.\n",
    "This vector representation can take one of the following three forms:\n",
    "* a binary representation,\n",
    "* an integer count,\n",
    "* and a float-valued weighted vector.\n",
    "\n",
    "To highlight the difference among the three approaches, we use a very simple example as follows:\n",
    "```\n",
    "    document_1: \"Data analysis is important.\"\n",
    "    document_2: \"Data wrangling is as important as data analysis.\"\n",
    "    document_3: \"Data science contains data analysis and data wrangling.\"\n",
    "```\n",
    "The three documents contain 20 tokens and 9 unique words.\n",
    "Those unique words are sorted alphabetically with total counts:\n",
    "```\n",
    "     'analysis': 3,\n",
    "     'and': 1,\n",
    "     'as': 2,\n",
    "     'contains': 1,\n",
    "     'data': 6,\n",
    "     'important': 2,\n",
    "     'is': 2,\n",
    "     'science': 1,\n",
    "     'wrangling': 2\n",
    "```\n",
    "Given the vocabulary above, \n",
    "both the binary and the integer count vectors are easy to compute.\n",
    "A binary vector stores 1s for the word that appears in a document and 0s for the other words in\n",
    "the vocabulary,\n",
    "whereas a count vector stores the frequency of each word appearing in the document.\n",
    "Thus, the binary vector representations for the three documents above are\n",
    "**Binary vector Table and Count vector Table.**\n",
    "   \n",
    "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
    "   |document 2:|1|0|1|0|1|1|1|0|1|\n",
    "   |document 3:|1|1|0|1|1|0|0|1|1|\n",
    "\n",
    "The count vector representations for the same documents would look as follows:\n",
    "\n",
    "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
    "   |document 2:|1|0|2|0|2|1|1|0|1|\n",
    "   |document 3:|1|1|0|1|3|0|0|1|1|\n",
    "\n",
    "Instead of using the two vector representations above, \n",
    "most existing text analysis algorithms, like document classification and information retrieval, \n",
    "prefer representing documents as weighted vectors.\n",
    "The raw term frequency is often replaced with a weighted term frequency\n",
    "that indicates how important a word is in a particular document.\n",
    "There are many different term weighting schemes online.\n",
    "To store each document as a weighted vector, we first need to choose a **weighting scheme**. \n",
    "\n",
    "**The most popular scheme is the TF-IDF weighting approach**. \n",
    "TF-IDF stands for term frequency-inverse document frequency. \n",
    "The **term frequency (TF)** for a word is the number of times the word appears in a document. \n",
    "In the preceding example, the term frequency in \"document _2\" (\"Data wrangling is as important as data analysis.\") for “data” is 2, since it appears twice in the document. \n",
    "\n",
    "Moreover, the **Document frequency** for a word is the number of documents that contain the word; \n",
    "it would also be 3 for “data” in the collection of the three preceding documents (document_1. document_2 and document_3). \n",
    "The Wikipidia entry on [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) lists \n",
    "a number of variants of TF-IDF. \n",
    "One variant is reproduced here\n",
    "$$tf\\cdot idf(w,d) = tf(w, d) * idf(w)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$tf(w,d)\\,=\\, \\sum_{i}^{|d|} 1_{w = w_{d,i}}$$\n",
    "and\n",
    "$$idf(w) = log\\left(\\frac{|D|}{|d \\in D: w \\in d |}\\right)$$\n",
    "\n",
    "The assumption behind TF-IDF is that words with high term frequency should receive high weight unless they also have high document frequency. \n",
    "Stopwords are the most commonly occurring words in the English language. They often occur many times within a single document, but they also occur in nearly every document. \n",
    "These two competing effects cancel out to give them low weights,\n",
    "as those very common words carry very little meaningful information about the actual contents of the document.\n",
    "Therefore, the TF-IDF weights for stopwords are almost always 0.\n",
    "With the TF-DF formulas above,\n",
    "the weighted vector representations for the example documents are computed as\n",
    "\n",
    "||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|0|0|0|0|0|0.176|0.176|0|0|\n",
    "   |document 2:|0|0|0.954|0|0|0.176|0.176|0|0.176|\n",
    "   |document 3:|0|0.477|0|0.477|0|0|0|0.477|0.176|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnlCEbDvWLpm"
   },
   "source": [
    "[link text](https://)Given the cleaned up Gutenberg documents, how can we generate those vectors for each documents? \n",
    "Unfortunately, NLTK does not implement methods that directly produce those vectors.\n",
    "Therefore, we will either write our own code to compute them or appeal to other data analysis libraries.\n",
    "Here we are going to use [scikit-learn](http://scikit-learn.org/stable/index.html), an open source machine \n",
    "learning library for Python.\n",
    "If you use Anaconda, you should already have scikit-learn installed, otherwise you will need to \n",
    "[install it](http://scikit-learn.org/stable/install.html) by following the instruction on its official website.\n",
    "\n",
    "Although scikit-learn features various classification, regression and clustering algorithms\n",
    "we are particularly interested in its feature extraction module, [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction).\n",
    "This module is often used to \"extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\" Please refer to its documentation on text feature extraction,\n",
    "section 4.2.3 of [Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). We will demonstrate the usage of the following two classes:\n",
    "* [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer): It converts a collection of text documents to a matrix of token counts. \n",
    "* [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "It converts a collection of raw documents to a matrix of TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XqmsoAxzUMD"
   },
   "source": [
    "### 2.1 Creating Count Vectors\n",
    "Let's start with generating the count vector representation for each Gutenberg document.\n",
    "Initialise the \"CountVector\" object: since we have pre-processed all the Gutenberg documents, \n",
    "the parameters, \"tokenizer\", \"preprocessor\" and \"stop_words\" are set to their default value, i.e., None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_xCOxhDWLpm"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE2h_CX8WLpp"
   },
   "source": [
    "Next, transform Gutenberg books into feature vectors. `fit_transform` does two things: First, it fits the model and learns the vocabulary; second it transforms the text data into feature vectors. \n",
    "Please note the input to `fit_transform` should be a list of strings. \n",
    "Since we have stored each tokenised article as a list of words, we concatenate all the words in the list and separate\n",
    "them with white spaces. \n",
    "The following code will do that:\n",
    "```python\n",
    "[' '.join(value) for value in tokenized_gutenberg.values()]\n",
    "```\n",
    "Then, we input this list of strings into `fit_transform`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHAqOGLmWLpp",
    "outputId": "f656717f-febe-4f4e-f38e-c2b34a5d3449"
   },
   "outputs": [],
   "source": [
    "data_features = vectorizer.fit_transform([' '.join(value) for value in tokenized_gutenberg.values()])\n",
    "print (data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiN-vbvbWLpr"
   },
   "source": [
    "The shape of document-by-word matrix should be 10788 * 17403. \n",
    "However, in order to save such a matrix in memory but also to speed up algebraic operations on the matrix,\n",
    "scikit-learn implements matrix/vector in a sparse representation.\n",
    "Let's check the count vector for the first article, i.e., 'training/1684'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQ1fKo1LWLpr",
    "outputId": "615e09a9-7f82-4a9f-f99a-56abf3860c03"
   },
   "outputs": [],
   "source": [
    "vocab2 = vectorizer.get_feature_names()\n",
    "for word, count in zip(vocab, data_features.toarray()[0]):\n",
    "    if count > 0:\n",
    "        print (word, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr2h2DfGWLpt"
   },
   "source": [
    "Another way to get the count list above is to use `FreqDist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCwT9HvaWLpt",
    "outputId": "50f0aa9c-317f-4a2f-ed55-15d19ad28ed4"
   },
   "outputs": [],
   "source": [
    "FreqDist(tokenized_gutenberg['austen-emma.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paPIzMOWWLpw"
   },
   "source": [
    "### 2.2 Creating TF-IDF Vectors\n",
    "Similar to the use of `CountVector`, we first initialise a `TfidfVectorizer` object by only specifying \n",
    "the value of \"analyzer\", and then covert the Gutenberg data into a list of strings, each of which corresponds\n",
    "to a Gutenberg books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jo-SzTEtWLpx",
    "outputId": "e4792f14-21b2-4ed9-b468-30a544a33752"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(analyzer = \"word\")\n",
    "tfs = tfidf.fit_transform([' '.join(value) for value in tokenized_gutenberg.values()])\n",
    "tfs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bZIF5dqWLpy"
   },
   "source": [
    "Let's print out the weighted vector for the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dq5wQyGeWLpz",
    "outputId": "9e9fb8f3-3891-4d42-defc-bd46885294ba"
   },
   "outputs": [],
   "source": [
    "vocab = tfidf.get_feature_names()\n",
    "for word, weight in zip(vocab, tfs.toarray()[0]):\n",
    "    if weight > 0:\n",
    "        print (word, \":\", weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4CZBz1iWLp1"
   },
   "source": [
    "So now we have converted all the Gutenberg books into feature vectors. \n",
    "We can use those vectors to, for example,\n",
    "* compute the similarity between two books, \n",
    "* search books for a given query\n",
    "* do other advance text analysis, such as document classification and clustering.\n",
    "\n",
    "Assume that we have a new document, how can we get its TF-IDF vector.\n",
    "We do this by using the transform function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-k7KD3oWLp1",
    "outputId": "bf0d0101-9c25-46c0-9cc5-851a2b11a119"
   },
   "outputs": [],
   "source": [
    "str = \"\"\"\n",
    "the former secretary of state hoped to win enough states to take a big step toward wrapping up her nomination fight\n",
    "with a democratic senator from Vermont.\n",
    "\"\"\"\n",
    "response = tfidf.transform([str])\n",
    "for col in response.nonzero()[1]:\n",
    "    print (vocab[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUfBB10WWLp3"
   },
   "source": [
    "Note that the text above is not included in the trained TF-IDF model with the 'transform' function, unless the `fit_transform` function is called,\n",
    "\n",
    "Both `CountVectorizer` and `TfidfVectorizer` come with their own options to automatically do pre-processing, tokenization, and stop word removal -- for each of these, instead of using their default value (i.e., None),\n",
    "we could customise the two vectorizer classes by either using a built-in method or specifying our own function.\n",
    "See the function documentation for more details.\n",
    "However, we wanted to write our own function for clean the text data in this chapter to show you how \n",
    "it's done step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKzr9nudWLqB"
   },
   "source": [
    "## Part 3. Extracting Other Features\n",
    "\n",
    "It is common for most text analysis tasks to treat documents as bags-of-words, which can significantly simplify the inference procedure of text analysis algorithms. \n",
    "However, things always have pros and cons. \n",
    "The bag-of-words representation loses lots of information encoded in either syntax or word order (i.e., dependencies between adjacent words in sentences.). \n",
    "For example, representing a document as a collection of unigrams effectively disregards any word order dependence,\n",
    "which fails to capture phrases and multi-word expressions. A similar issue has been mentioned in section 2.1 Creating Count Vectors. \n",
    "\n",
    "In this section, we are going to show you how to\n",
    "* use Part-of-Speeching (POS) tagging to extract specific word groups, such as all nouns, verbs, etc.,\n",
    "* extract n-grams,\n",
    "*  and extract collocations\n",
    "\n",
    "These features can be further used to enrich the representation of a document.\n",
    "\n",
    "### 3.1 Extracting Nouns and Verbs\n",
    "\n",
    "It is easy for human to tell the difference between nouns, verbs, \n",
    "adjectives and adverbs, as we have learnt them back in elementary school.\n",
    "However, how can we automatically classify words into their parts of speech (i.e., lexical categories or word classes) \n",
    "and label them accordingly with computer program? \n",
    "This section is not going to discuss how to determine the category of a word from a linguistic perspective.\n",
    "Instead it demonstrates the use of some existing POS taggers to extract words in a specific lexical category.\n",
    "It has been proven that words together with their part-of-speech (POS) are quite useful for many language processing tasks. \n",
    "\n",
    "In NLP, the process of labelling words with their corresponding part-of-speech (POS) tags is known as [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging).\n",
    "A POS tagger processes a sequence of words and attaches a POS tag to each word based on both its definition and its context. There are many POS taggers available online, such as [Sandford POS tagger](http://nlp.stanford.edu/software/tagger.shtml). \n",
    "We are going to use the one implemented by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dz4uorTZWLqC",
    "outputId": "5ec5b92c-7423-4f00-e52a-a230d7793874"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "example_sent = 'A POS tagger processes a sequence of words and attaches a POS tag to each \\\n",
    "word based on both its definition and its context'\n",
    "\n",
    "text = nltk.word_tokenize(example_sent)\n",
    "\n",
    "tagged_sent = nltk.tag.pos_tag(text)\n",
    "\n",
    "print (tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mavxTUGuWLqD"
   },
   "source": [
    "If you are seeing these tags for the first time, you will wonder what these tags mean. \n",
    "You can find the specification of all the tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). \n",
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLsb9L60WLqD",
    "outputId": "453f3bf8-050f-4afd-d1a4-95e02f352b91"
   },
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "print ('\\n\\n', nltk.help.upenn_tagset('NNP'))\n",
    "print ('\\n\\n', nltk.help.upenn_tagset('IN'))\n",
    "print ('\\n\\n', nltk.help.upenn_tagset('PRP$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlJWSXIdWLqE"
   },
   "source": [
    "The example sentence has been processed by `pos_tag` into a list of tuples, each of which is a pair of a word and its POS tag. We see that 'a' is 'DT', a determiner; 'its' is 'PRP$', a possessive pronoun; 'and' is 'CC', a coordinating conjunction, 'words' is 'NNS', a noun in the plural form, and so on. Note that several of the corpora included in NLTK have been tagged for their POS. Please click [here](http://www.nltk.org/howto/corpus.html#tagged-corpora) to see how to access those tagged corpora.\n",
    "Here is an example of using the `tagged_words` function to retrieve all words in Brown corpus with their tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ex3eDVuyWLqF",
    "outputId": "9c03c1c7-f7cc-4192-b636-a7e61ab301ab"
   },
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsqzwN50WLqH"
   },
   "source": [
    "Please note that the collection of tags is known as a tag set. \n",
    "There are many different conventions for tagging words.\n",
    "Therefore, tag sets can vary among different tasks.\n",
    "What we used above is the Penn Treebank tag set.\n",
    "Let's change the tag set to the Universal POS tag set, and print the Brown corpus again.\n",
    "You will find different tags are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oYkPfeWWLqH",
    "outputId": "ec34c913-9d88-40cf-9692-8426cc2d8bb8"
   },
   "outputs": [],
   "source": [
    "nltk.download('universal_tagset')\n",
    "nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_YE5oS8WLqI"
   },
   "source": [
    "If you would like to learn more about POS tagging, please refer to \"1. Categorizaing and Tagging Words\" in the Part 6 Reading Materials.\n",
    "\n",
    "Given the tagged text, you can easily identify all the nouns, verbs, etc.\n",
    "Nouns generally refer to people, places, things, or concepts, e.g., Monash, Melbourne, university, data, and science. \n",
    "Nouns can appear after determiners and adjectives, and can be the subject or object of the verb.\n",
    "Now how can we extract all the nouns from a text?\n",
    "Assume we use the Penn Treebank tag set.\n",
    "Here are all the tags for nouns:\n",
    "```\n",
    "    NN    Noun, singular or mass\n",
    "    NNS   Noun, plural\n",
    "    NNP   Proper noun, singular\n",
    "    NNPS  Proper noun, plural\n",
    "```\n",
    "It is not hard to see all the tags above start with 'NN'.\n",
    "Thus, we can iterate over all the words and check if their tag string starts with 'NN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBHwQaCVWLqI",
    "outputId": "7840fd3e-1df7-4e05-fda4-bc4ed9d69036"
   },
   "outputs": [],
   "source": [
    "all_nouns = [w for w,t in tagged_sent if t.startswith('NN')]\n",
    "all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-_YMszWWLqK"
   },
   "source": [
    "Similarly, you will find that all the verb tags start with 'VB', see\n",
    "```\n",
    "    VB\tVerb, base form\n",
    "    VBD   Verb, past tense\n",
    "    VBG   Verb, gerund or present participle\n",
    "    VBN   Verb, past participle\n",
    "    VBP   Verb, non-3rd person singular present\n",
    "    VBZ   Verb, 3rd person singular present\n",
    "```\n",
    "Thus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNTBQvtbWLqK",
    "outputId": "0cef3c6e-8144-4551-91a7-49b65f9e7674"
   },
   "outputs": [],
   "source": [
    "all_verbs = [w for w,t in tagged_sent if t.startswith('VB')]\n",
    "all_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIUcMaYZWLqL"
   },
   "source": [
    "Unfortunately, the Gutenberg corpus that we have been using, has no built-in POS tags. But you can get sentences from Gutenberg corpus, and then you can get the POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHuEFK-HWLqL"
   },
   "source": [
    "### 3.2 Extracting N-grams and Collocations\n",
    "\n",
    "Besides unigrams that we have been working on so far,\n",
    "N-grams of texts are also extensively used in various text analysis tasks.\n",
    "They are basically contiguous sequences of `n` words from a given sequence of text.\n",
    "When computing the n-grams you typically move a fixed size window of size n\n",
    "words forward.\n",
    "For example, for the sentence\n",
    "\"Laughter is like a windshield wiper.\"\n",
    "if N = 2 (known as bigrams), the n-grams would be:\n",
    "```\n",
    "    Laughter is \n",
    "    is like \n",
    "    like a \n",
    "    a windshield \n",
    "    windshield wiper\n",
    "```\n",
    "So you have 5 bigrams in this case. Notice that the generative process above\n",
    "essentially moves one word forward to generate the next bigram.\n",
    "If N = 3 (known as trigrams), the n-grams would be:\n",
    "```\n",
    "    Laughter is like \n",
    "    is like a \n",
    "    like a  windshield\n",
    "    a  windshield wiper\n",
    "```\n",
    "What are N-grams used for? They can be used to build n-gram language model that\n",
    "can be further used for speech recognition, spelling correction, entity detection, etc.\n",
    "In terms of text mining tasks, n-grams is used for developing features for \n",
    "classification algorithms, such as SVMs, MaxEnt models, Naive Bayes, etc.\n",
    "The idea is to expand the unigram feature space with n-grams.\n",
    "But please notice that\n",
    "the use of bigrams and trigrams in your feature space may not necessarily yield significant performance\n",
    "improvement. The only way to know this is to try it! \n",
    "Extracting from a text a list of n-gram can be easily accomplished with function `ngram()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3vULkYyWLqL"
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "bigrams = ngrams(gutenberg.words(), n = 2)\n",
    "fdbigram = FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFyc_jFaWLqQ",
    "outputId": "fb19c15e-b733-4b8a-d19d-382e46dbd874"
   },
   "outputs": [],
   "source": [
    "fdbigram.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYXr3VqDWLqR"
   },
   "source": [
    "Collocations are expressions of multiple words that commonly co-occur. \n",
    "\n",
    ">Finding collocations requires first calculating the frequencies of words and\n",
    "their appearance in the context of other words. Often the collection of words\n",
    "will then requiring filtering to only retain useful content terms. Each ngram\n",
    "of words may then be scored according to some association measure, in order\n",
    "to determine the relative likelihood of each ngram being a collocation. (Quoted from [here](http://www.nltk.org/_modules/nltk/collocations.html))\n",
    "\n",
    "For example, to extract bigram collocations, we can firstly extract bigrams then get the commonly co-occurring ones by ranking the bigrams by some measures. A commonly used measure is [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI). The following code will find the best 50 bigrams using the PMI scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrpS4YgNWLqR",
    "outputId": "afcda4d8-42b3-45c2-e7cf-8a0df12b6c28"
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(gutenberg.words())\n",
    "finder.nbest(bigram_measures.pmi, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEPNDoOpWLqS"
   },
   "source": [
    "The `collocations` module implements a number of measures to score collocations or other associations. \n",
    "They include Student's t test, Chi-Square, likelihood ratios, PMI and so on.\n",
    "Here we used PMI scores for finding bigrams.\n",
    "Please read \"2. Collocations\" on the Part 5 Reading Materials for a detailed tutorial on finding collocations with NLTK.\n",
    "If you would like to know more about collocations, please refer to \"3. Collocations\" on the Part 5 Reading Materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBV-V9tWLqS"
   },
   "source": [
    "## Part 4. Summary\n",
    "\n",
    "This chapter has show you how to \n",
    "\n",
    "* generate vocabulary be further exploring the tokenized text with some simple statistics. \n",
    "* convert unstructured text to structured form using the bag-of-words model\n",
    "* compute TF-IDF\n",
    "* extract words in specific lexical categories, n-grams and collocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJGgyp2KWLqT"
   },
   "source": [
    "## Part 5. Reading Materials\n",
    "\n",
    "1. \"[Categorizaing and Tagging Words](http://www.nltk.org/book/ch05.html)\", \n",
    "Chapter 5 of \"Natural Language Processing with Python\".\n",
    "2. \"[Collocations](http://www.nltk.org/howto/collocations.html)\": An NTLK tutorial on how to extract collocations 📖 .\n",
    "3. \"[Collocations](http://nlp.stanford.edu/fsnlp/promo/colloc.pdf)\": An introduction to collocation by Manning and Schutze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOsq3a2_WLqT"
   },
   "source": [
    "## Part 6 . Exercises\n",
    "\n",
    "1. We have shown you how to generate frequency of frequency bar chart with term frequency. Similarly, you can generate the bar chart based on document frequency. \n",
    "2. Remove short words. There are some very short words in the vocabulary, for example, 'aa', 'ab', 'ad', 'ax', etc.\n",
    "Write Python code to explore the distribution of word lengths, and remove those words with less than two characters.\n",
    "3. Write code to tag the Gutenberg corpus with the Penn Treebank tag set, find the top 10 most common tags, nouns, and verbs.\n",
    "4. There might be some text analysis tasks where the binary occurrence markers might be enough. \n",
    "Please modify the CountVectorizer code to generate binary vectors for all the Gutenberg books. \n",
    "5. We have shown you how to generate feature vectors from raw text. As we mentioned before, you can actually customize the two vectorizer classes by specifying, for example, the tokenizer and stopword list. So try\n",
    "to customize either vecotorizer so that it can carry out all the steps."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SIT742P04B-TextFeatures.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
