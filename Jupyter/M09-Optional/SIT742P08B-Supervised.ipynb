{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncX4IZ-QKI8U"
   },
   "source": [
    "# SIT742: Big Data Analytics \n",
    "**(Week 08: Data Analytics (I))**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Session 8B - Supervised Learning\n",
    "\n",
    "\n",
    "The purpose of this session is to demonstrate different coefficient and linear regression.\n",
    "\n",
    "\n",
    "## Content\n",
    "\n",
    "### Part 1 Data Dependency\n",
    "\n",
    "1.1 Pearson's-r Correlation coefficient\n",
    "\n",
    "1.2 Spearman's rank coefficient\n",
    "\n",
    "\n",
    "### Part 2 Linear Regression\n",
    "\n",
    "2.1 Multiple Linear Regression\n",
    "\n",
    "2.2 Regression for Median House Price\n",
    "\n",
    "### Part 3 Distances\n",
    "\n",
    "3.1 Euclidean Distance\n",
    "\n",
    "3.2 Cosine Distance\n",
    "\n",
    "3.3 Term-by-Document Matrix\n",
    "\n",
    "### Part 4 K-NN Classification\n",
    "\n",
    "4.1 K-NN in Python\n",
    "\n",
    "4.2 Decision Boundary\n",
    "\n",
    "### Part 5 Naive Bayes Classifier\n",
    "\n",
    "5.1 NBC by Example\n",
    "\n",
    "5.2 NBC Exercise\n",
    "\n",
    "### Part 6 Random Forest\n",
    "\n",
    "6.1 Decision Trees\n",
    "\n",
    "6.2 Decision Trees and over-fitting\n",
    "\n",
    "6.3 Ensembles of Estimators: Random Forests\n",
    "\n",
    "6.4 Random Forest Regressor\n",
    "\n",
    "6.5 Random Forest Limitations\n",
    "\n",
    "\n",
    "### Part 7 Confidence Interval\n",
    "\n",
    "7.1 Population and Sample\n",
    "\n",
    "7.2 Confidence Interval\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRwy1I_GKI8d"
   },
   "source": [
    "# 1.Data Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccLW-Shkl3B3"
   },
   "source": [
    "## 1.1.Pearson's-r Correlation coefficient\n",
    "\n",
    "\n",
    "We assume $X=\\left\\{ X_{1},\\ldots,X_{n}\\right\\}$ \n",
    "and $Y=\\left\\{ Y_{1},\\ldots,Y_{n}\\right\\}$. Then Pearson-r correlation coefficient is defined as \n",
    "\n",
    "$$ \\rho(X,Y) = \\frac{\\text{cov}(X,Y)}{\\sigma_X \\sigma_Y} =  \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n(X_i-\\bar{X})^2} \\sqrt{\\sum_{i=1}^n(Y_i-\\bar{Y})^2}} $$\n",
    "\n",
    "Use the car data and find the Pearson's-r correlation coefficient between car weights and fuel consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yh41sn3OKI8h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozgxW31YKI8u"
   },
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cv1BPC0nKI83"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/Auto.csv'\n",
    "DataSet = wget.download(link_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9f9093mKI8-"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Auto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lvlHSGeKI9H"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6EWzvycKI9P"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ba6MJhmWKI9X"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdufSUZwKI9f"
   },
   "outputs": [],
   "source": [
    "miles = data['miles']\n",
    "weights = data['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjwXJw1KKI9r"
   },
   "outputs": [],
   "source": [
    "print(miles[:10])\n",
    "print(weights[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJePPORcKI95"
   },
   "outputs": [],
   "source": [
    "pearson_r = np.cov(miles, weights)[0, 1] / (miles.std() * weights.std())\n",
    "print(pearson_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Jsh7-zHKI-J"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(miles,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeoTpLl4KI-S"
   },
   "outputs": [],
   "source": [
    "horse = data['Horse power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lF8vUDYuKI-f"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(weights,horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtqLr_qQKI-o"
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(7, 5), dpi=100)\n",
    "ax.scatter(weights,miles, alpha=0.6, edgecolor='none', s=100)\n",
    "ax.set_xlabel('Car Weight (tons)')\n",
    "ax.set_ylabel('Miles Per Gallon')\n",
    "\n",
    "line_coef = np.polyfit(weights, miles, 1)\n",
    "xx = np.arange(1, 5, 0.1)\n",
    "yy = line_coef[0]*xx + line_coef[1]\n",
    "\n",
    "ax.plot(xx, yy, 'r', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gn_pega1KI-v"
   },
   "source": [
    "**Exercise 1**: \n",
    "\n",
    "1. Find the Pearson's-r coefficient for two linearly dependent variables. Add some noise and see the effect of varying the noise. \n",
    "2. Simulate and visualize some data with positive linear correlation\n",
    "3. Simulate and visualize some data with negative linear correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vl-q7Z3XKI-y"
   },
   "outputs": [],
   "source": [
    "xx = np.arange(-5, 5, 0.1)\n",
    "pp = 1.5  # level of noise\n",
    "yy = xx + np.random.normal(0, pp, size=len(xx))\n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='none')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "\n",
    "line_coef = np.polyfit(xx, yy, 1)\n",
    "line_xx = np.arange(-5, 5, 0.1)\n",
    "line_yy = line_coef[0]*line_xx + line_coef[1]\n",
    "\n",
    "ax.plot(line_xx, line_yy, 'b', lw=2)\n",
    "\n",
    "print(scipy.stats.pearsonr(xx, yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "or4EIreJKI-5"
   },
   "source": [
    "Pearson's r coefficient is limited to analyze the linear correlation between two variables. It is not capable to show the non-linear dependency. Investigate the Pearson's r coefficient between two variables that are correlated non-linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiQIhPzuKI-8"
   },
   "outputs": [],
   "source": [
    "# generate some data, first for X\n",
    "xx = np.arange(-5, 5, 0.1)\n",
    "\n",
    "# assume Y = 2Y + some perturbation\n",
    "pp = 1.1  # level of noise\n",
    "yy = xx**2 + np.random.normal(0, pp, size=len(xx))\n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='b')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "ax.set_title('$Y = X^2+\\epsilon$', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6O3Dd46oKI_D"
   },
   "outputs": [],
   "source": [
    "# generate some data, first for X\n",
    "xx = np.arange(-5, 5, 0.1)\n",
    "\n",
    "# assume Y = 2Y + some perturbation\n",
    "pp = 1.1  # level of noise\n",
    "yy = xx**2 + np.random.normal(0, pp, size=len(xx))\n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='b')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "ax.set_title('$Y = X^2+\\epsilon$', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5f44ehqKI_L"
   },
   "source": [
    "The Pearson's-r correlation is near zero which means there is no linear correlation. But how about non-linear correlation? Isn't $y=x^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIvvxJq_KI_R"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAqwNwEHKI_a"
   },
   "source": [
    "<a id = \"spearman\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.2 Spearman's rank coefficient</span>\n",
    "\n",
    "Spearman's rank coefficient is used for discrete/ordinal data. Find the Spearman's rank between horse power and number of cylinders of the car data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ooNL0lBbKI_d"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G5UUnUQKI_n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#horse = np.array([float(dd[4]) for dd in data[1:]])\n",
    "#cylinder = np.array([float(dd[2]) for dd in data[1:]])\n",
    "horse = data['Horse power']\n",
    "cylinder = data['cylinder number']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5), dpi=100)\n",
    "ax.scatter(horse, cylinder, alpha=0.6, edgecolor='none', s=100)\n",
    "ax.set_xlabel('Horse power')\n",
    "ax.set_ylabel('#Cylinders')\n",
    "\n",
    "print(scipy.stats.spearmanr(horse, cylinder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtrH93OfKI_v"
   },
   "source": [
    "**Exercise 2**. \n",
    "Compute the spearman rank correlation between \"Horse power\" and \"Engine displacement\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6ab6U6aKI_x"
   },
   "source": [
    "<details><summary><font color=\"blue\"><b>Click here for solutions to exercise</b></font></summary>\n",
    "```\n",
    "displacement = data['Engine displacement']\n",
    "scipy.stats.spearmanr(horse,displacement)\n",
    "\n",
    "radius = 150*np.ones(len(horse))\n",
    "fig, ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "ax.scatter(displacement, horse, alpha=0.2, c='m', edgecolor='none',s=radius)\n",
    "\n",
    "ax.set_xlabel('Engine displacement')\n",
    "ax.set_ylabel('Horse power')\n",
    "plt.savefig(\"Engine_vs_horse.pdf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYdyubfLKI_z"
   },
   "source": [
    "# 2.Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-aKXRa2KI_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pt0z5IwTKI_-"
   },
   "source": [
    "First we investigate a simple case by fitting a linear regression for three data points. First we simulate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioTM5Ss-KJAB"
   },
   "outputs": [],
   "source": [
    "# simulating the data\n",
    "\n",
    "x = np.c_[0, 1, 2, 1.5].T\n",
    "y  = [1, 1.5, 3.1, 1.5]\n",
    "\n",
    "print (x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qo5lZIHpKJAJ"
   },
   "outputs": [],
   "source": [
    "#plotting the data\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "ax.scatter(x, y, c='r')\n",
    "ax.set_title('simulated data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54W-YMmFKJAQ"
   },
   "source": [
    "Now we fit the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMH9F5_4KJAS"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# instanciate the model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o--wX15OKJAe"
   },
   "outputs": [],
   "source": [
    "print (\"Coefficients:\", lr.coef_)\n",
    "print (\"Intercept:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZJ-oYgTKJAn"
   },
   "source": [
    "Let's plot the line to see how it estimates our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvQXH5PgKJAq"
   },
   "outputs": [],
   "source": [
    "yhat = lr.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "ax.scatter(x, y, c='r')\n",
    "ax.plot(x, yhat)\n",
    "\n",
    "ax.set_title('simulated data and the estimated line')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9JWr5ydKJAz"
   },
   "source": [
    "We can use the method `predict()` to predict `y` for a new `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8otmQurMKJA4"
   },
   "outputs": [],
   "source": [
    "x_test = np.c_[4, 2.3].T\n",
    "y_test = lr.predict(x_test)\n",
    "\n",
    "print (x_test.T)\n",
    "print (y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8F6xXeK4KJBD"
   },
   "source": [
    "## 2.1.Multiple Linear Regression\n",
    "\n",
    "\n",
    "Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data. Every value of the independent variable x is associated with a value of the dependent variable y. For example if we have two explanatory variables (attributes, features), our data has such a form:\n",
    "\n",
    "$$\n",
    "D=\\left\\{ \\left(\\left(x_{1,1},x_{2,1}\\right),y_{1}\\right),\\left(\\left(x_{1,2},x_{2,2}\\right),y_{2}\\right),\\ldots,\\left(\\left(x_{1,n},x_{2,n}\\right),y_{n}\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "Now we fit a multiple linear regression $y = x_1 + 2x_2 + 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAHTVCcdKJBF"
   },
   "outputs": [],
   "source": [
    "# simulate the data\n",
    "x = np.c_[[0, 0], [0, 1], [1, 1], [1, 0]].T\n",
    "y = [1.5, 3.2, 4, 2]\n",
    "\n",
    "print (x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJcJyRQpKJBN"
   },
   "outputs": [],
   "source": [
    "mlr = linear_model.LinearRegression(fit_intercept=True)\n",
    "mlr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IxnKlDgKJBU"
   },
   "outputs": [],
   "source": [
    "print (mlr.coef_)\n",
    "print (mlr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee36NSp9KJBf"
   },
   "outputs": [],
   "source": [
    "print (mlr.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqkknlDDKJBo"
   },
   "source": [
    "**Exercises 3**: \n",
    "\n",
    "As the score suggests, now we have the perfect regression. Change the values of $y$ slightly and see what effect it has on the `mlr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkemegxvKJBr"
   },
   "source": [
    "<details><summary><font color=\"blue\"><b>Click here for solutions to exercise</b></font></summary>\n",
    "```\n",
    "x = np.c_[[0, 0], [0, 1], [1, 1], [1, 0]].T\n",
    "y = [2, 3, 4.1, 2.3]\n",
    "\n",
    "print (x)\n",
    "print (y)\n",
    "    \n",
    "mlr = linear_model.LinearRegression(fit_intercept=True)\n",
    "mlr.fit(x,y)\n",
    "\n",
    "print (mlr.coef_)\n",
    "print (mlr.intercept_)\n",
    "print (mlr.predict(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2N41YcpKJBu"
   },
   "source": [
    "## 2.2 Regression for median house prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5otpFK-2KJBx"
   },
   "source": [
    "We are going to use the package `pandas` for reading and storing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYCWcS_aKJBy"
   },
   "outputs": [],
   "source": [
    "wget.download('https://github.com/tuliplab/mds/raw/master/Jupyter/data/housing_300.csv')\n",
    "\n",
    "data = pd.read_csv('housing_300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq_hNvfFKJB5"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDywbMn8KJCA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ys_8kL4LKJCH"
   },
   "source": [
    "Plot the scatter plot of the number of rooms vs the median house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3hq4DvNKJCJ"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7), dpi=100)\n",
    "median_prices = data['MEDV']\n",
    "avg_rooms = data['RM']\n",
    "scales = 50*np.ones(len(median_prices))\n",
    "ax.scatter(avg_rooms, median_prices, color='b',s=scales, alpha=0.7, edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gpq1d5vKJCU"
   },
   "outputs": [],
   "source": [
    "print (avg_rooms.shape)\n",
    "print (median_prices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhbSkmO8KJCb"
   },
   "source": [
    "How correlated are the number of rooms and the price of the house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g73oQWhTKJCc"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(avg_rooms, median_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtnPWqK1KJCk"
   },
   "source": [
    "Now we want to fit a linear regression mode on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTtqbbwTKJCm"
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "x = np.c_[avg_rooms.values]\n",
    "y = median_prices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APNDJYZ4KJCs"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGtxGQ-nKJC7"
   },
   "outputs": [],
   "source": [
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYCehFbUKJDK"
   },
   "outputs": [],
   "source": [
    "print (lr.coef_)\n",
    "print (lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXMmvsULKJDa"
   },
   "outputs": [],
   "source": [
    "# obtain the model parameters\n",
    "print (lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gy4_A2FOKJDq"
   },
   "outputs": [],
   "source": [
    "# predict \n",
    "\n",
    "yhat = lr.predict(x)\n",
    "print (x[:10])\n",
    "print (yhat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QazFLh7SKJDx"
   },
   "outputs": [],
   "source": [
    "#plot the result\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7),dpi=100)\n",
    "\n",
    "scales = 20*np.ones(len(median_prices))\n",
    "ax.scatter(avg_rooms,median_prices,color='b',s=scales,alpha=0.7,edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')\n",
    "\n",
    "# plot the regression linear leared\n",
    "ax.plot(x,yhat)\n",
    "\n",
    "# visualize the residuals\n",
    "tmp = np.reshape(x,[1,len(x)])[0]\n",
    "tmp_x = []\n",
    "tmp_y = []\n",
    "for i in range(len(x)):\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,y[i])\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,yhat[i])\n",
    "    ax.plot(tmp_x,tmp_y,color='g',linewidth=0.5)\n",
    "    tmp_x = []\n",
    "    tmp_y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNB46J4sKJEC"
   },
   "source": [
    "It is customary to test your model on **unseen** data. So we divide our data into two parts. We use 70% of it to train the model and 30% to evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xTpgEjYKJEE"
   },
   "outputs": [],
   "source": [
    "split = 0.7\n",
    "split_idx = int(np.round(split * len(data)))\n",
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSgAbdjMKJEK"
   },
   "outputs": [],
   "source": [
    "train_data = data[0:200]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXsrzTbLKJER"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "train_data.plot(kind='scatter', x='RM', y='MEDV', ax=axs[0], figsize=(7, 7))\n",
    "train_data.plot(kind='scatter', x='AGE', y='MEDV', ax=axs[1], figsize=(7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9TmkjdRKJEZ"
   },
   "outputs": [],
   "source": [
    "test_data = data[200:300]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bu3itlHpKJEo"
   },
   "outputs": [],
   "source": [
    "train_X = train_data['RM'].values\n",
    "train_X = np.c_[train_X]\n",
    "train_Y = train_data['MEDV'].tolist()\n",
    "\n",
    "test_X = test_data['RM'].values\n",
    "test_X = np.c_[test_X]\n",
    "test_Y = test_data['MEDV'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrMKRA9SKJEu"
   },
   "outputs": [],
   "source": [
    "print (type(train_X))\n",
    "print (train_X.shape)\n",
    "print (type(train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fPmn9gJKJE0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Build a linear regression model from training data\n",
    "'''\n",
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "lr.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJu_VnU3KJFA"
   },
   "outputs": [],
   "source": [
    "print (lr.coef_)\n",
    "print (lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpFKfwgGKJFJ"
   },
   "source": [
    "Now we plot the linear regression result and the data to see how it fits the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6DbyZgdKJFL"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7),dpi=100)\n",
    "\n",
    "# plot training data\n",
    "scales = 20*np.ones(len(train_Y))\n",
    "ax.scatter(train_X,train_Y,color='b',s=scales,alpha=0.7,edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')\n",
    "plt.title('Training a simple linear regression model')\n",
    "\n",
    "# plot the regression line\n",
    "train_Yhat = lr.predict(train_X)\n",
    "plt.plot(train_X,train_Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_iYlZG7KJFV"
   },
   "source": [
    "Now that we have obtained the model parameters, we can use the model to predict for unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btfMWqLNKJFY"
   },
   "outputs": [],
   "source": [
    "yhat_test = lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CsZt6DxKJFd"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7),dpi=100)\n",
    "\n",
    "# plot the predicted points along the prediction line\n",
    "scales = 30*np.ones(len(test_X))\n",
    "ax.scatter(test_X,yhat_test,s=scales,color='b',edgecolor='r')\n",
    "ax.plot(test_X,yhat_test,color='b',linewidth=.2)\n",
    "\n",
    "# plot the true values\n",
    "scales = 30*np.ones(len(test_X))\n",
    "ax.scatter(test_X,test_Y,s=scales,color='g',edgecolor='b')\n",
    "\n",
    "# plot the residual line\n",
    "tmp = np.reshape(test_X,[1,len(test_X)])[0]\n",
    "tmp_x = []\n",
    "tmp_y = []\n",
    "for i in range(len(test_X)):\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,yhat_test[i])\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,test_Y[i])\n",
    "    ax.plot(tmp_x,tmp_y,color='red',linewidth=0.5)\n",
    "    tmp_x = []\n",
    "    tmp_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IJUmL88KJFj"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0BgOpnlKJFr"
   },
   "source": [
    "# 3.Distances\n",
    "\n",
    "`Distance` is a numerical description of how far apart objects are. It is a concrete way of describing what it means for elements of some space to be close or far away from each other, for example the distance between two vectors in an 2-dimensional space.\n",
    "\n",
    "Now that you have know how to represent an n-dimensional vector in Python with NumPy arrays, we will write a function as a metric to measure the distance between two vectors. There are multiple ways to measure the distance between two vectors. We will discuss Euclidean distance and cosine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "BSZVWk4cKJFz"
   },
   "source": [
    "## 3.1.Euclidean Distance\n",
    "\n",
    "Euclidean distance comes from Geometry. If we assume $\\mathbf{x}_{1}=\\left[x_{11},x_{12},\\ldots,x_{1n}\\right]$ and $\\mathbf{x}_{2}=\\left[x_{21},x_{22},\\ldots,x_{2n}\\right]$, then the Euclidean distance between $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ is defined as:\n",
    "\n",
    "$$d\\left(\\mathbf{x}_{1},\\mathbf{x}_{2}\\right)=\\sqrt{\\left(x_{11}-x_{21}\\right)^{2}+\\left(x_{12}-x_{22}\\right)^{2}+\\ldots+\\left(x_{1n}-x_{2n}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "We can use array operators for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIytlQaCKJF1"
   },
   "outputs": [],
   "source": [
    "x1 = np.array([2, 5, 4, 6, 8])\n",
    "x2 = np.array([3, 5, 6, 8, 6])\n",
    "\n",
    "print (x1 - x2)\n",
    "print ((x1 - x2) ** 2)\n",
    "print (np.sqrt(np.sum((x1 - x2) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50Eeh3aHKJF8"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjxbLMD1KJGC"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance1(x1, x2):\n",
    "    d = x1 - x2\n",
    "    d = d ** 2\n",
    "    return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TIEnYjfKJGJ"
   },
   "outputs": [],
   "source": [
    "x1 = np.array([-1, 2, 0, 5])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "\n",
    "print (euclidean_distance1(x1, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LcAKVdsgKJGP"
   },
   "source": [
    "Since two vectors passed to the function should be the same size, it is better to perform a sanity check before applying the subtraction. Otherwise it will raise an error. We can do this by using `if - elif` statement or as a better practice by using `try - except`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxRzf0u9KJGU"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def euclidean_distance2(x1, x2):\n",
    "    if x1.shape[0] != x2.shape[0]:\n",
    "        sys.exit('x1 and x2 are not the same size')\n",
    "    else:\n",
    "        d = x1 - x2\n",
    "        d = d ** 2\n",
    "        return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKJjPSHMKJGa"
   },
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0, 1])\n",
    "euclidean_distance2(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fX4Yc-goKJGf"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance3(x1, x2):\n",
    "    try:\n",
    "        d = x1 - x2\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print (\"Vectors passed to the function are not the same size\")\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRlue1AqKJGt"
   },
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 2])\n",
    "a = euclidean_distance3(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1tn8Y8jKJG1"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance4(x1, x2):\n",
    "    try:\n",
    "        d = np.array(x1) - np.array(x2)\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print (\"Vectors passed to the function are not the same size\")\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVqx91NIKJG6"
   },
   "source": [
    "## 3.2.cosine similarity and distance\n",
    "\n",
    "Cosine similarity is a measure of similarity between two vectors based on the angle between them. Cosine similarity is widely used in information retrieval and text mining as a measure of similarity between documents and is defined as:\n",
    "\n",
    "$$S_{c}\\left(\\mathbf{x}_{1},\\mathbf{x_{2}}\\right)=\\frac{\\mathbf{x}_{1}.\\mathbf{x_{2}}}{\\parallel\\mathbf{x}_{1}\\parallel^{2}+\\parallel\\mathbf{x}_{2}\\parallel^{2}-\\mathbf{x}_{1}.\\mathbf{x_{2}}}$$\n",
    "\n",
    "\n",
    "Cosine similarity is particularly used in positive space where the outcome is bounded in [0, 1]. The cosine distance is defined as the complement to cosine similarity in positive space that is $D_{c}\\left(x_{1},x_{2}\\right)=1-S_{c}\\left(x_1,x_2\\right)$ where $D_c$ is the cosine distance and $S_c$ is the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2H-aW6UlKJG9"
   },
   "outputs": [],
   "source": [
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([3,4,6])\n",
    "\n",
    "x1 * x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6M_NhKLKJHB"
   },
   "outputs": [],
   "source": [
    "def cosine_distance(x1, x2):\n",
    "    try:\n",
    "        num = (x1*x2).sum()\n",
    "        denom = (x1*x1).sum() + (x2*x2).sum() - (x1*x2).sum()\n",
    "        num += 0.0    # or use np.astype(float) to make sure of float division\n",
    "        return 1 - num/denom\n",
    "    except ValueError as e:\n",
    "        print (\"Vectors passed to the function are not the same size\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cu_2tepxKJHK"
   },
   "outputs": [],
   "source": [
    "x1 = np.array([2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "cosine_distance(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjwIlMImKJHW"
   },
   "source": [
    "## 3.3.Term-by-Document matrix\n",
    "\n",
    "A term-by-document matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the document collection. Each row corresponds to a document and each column correspond to a term. Thus the value that appears in row $j$ and column $i$ represents the frequency of appearing term $i$ in document $j$.\n",
    "\n",
    "We will represent two datasets with term-by-document matrix:\n",
    "\n",
    "* a collection of 100 Twitter messages about Geelong\n",
    "* a collection of 6 news articles (5 about Apple and 1 about politics)\n",
    "\n",
    "The data is already collected and stores in text files. Thus you will need to:\n",
    "\n",
    "* read the text files\n",
    "    * using file object\n",
    "* perform pre-processing\n",
    "    * using string methods\n",
    "    * using re package\n",
    "* construct the term-by-document matrix\n",
    "    * using numpy arrays and operations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sdW9Mb1KJHZ"
   },
   "source": [
    "### 3.3.1.Twitter dataset\n",
    "First read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ron5lQO9KJHc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "wget.download('https://github.com/tuliplab/mds/raw/master/Jupyter/data/tweets.txt')\n",
    "\n",
    "# join the subdirectory of the data and data file name\n",
    "file_path = os.path.join(\"tweets.txt\")\n",
    "\n",
    "# read the contents of the file and store it in a list\n",
    "with open(file_path) as fp:\n",
    "    tweets = fp.readlines()    \n",
    "for tweet in tweets:\n",
    "    print (tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WH9ijnr_KJHp"
   },
   "source": [
    "Mostly when dealing with data, we have to perform some sort of data pre-processing. Data collection is often loosely controlled, resulting in out of the range values, missing values, and etc. Thus quality of the data is first and formost before running an analysis. This step is specific to the nature of the data. For example for text data it may consist of cleaning, normalization, tokenization, and etc. \n",
    "\n",
    "In this case, our pre-processing consists of:\n",
    "\n",
    "* converting all the words into lower case to remove the effect of the letter case\n",
    "* replacing the URLs with a simple string such as 'url'. From the previous cell, you should be able to see that many of the tweets contain a URL. Since we are not using them now, we can remove them or replace them.\n",
    "* Removing the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsIxr50TKJHr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import codecs\n",
    "\n",
    "def pre_process(doc):\n",
    "    \"\"\"\n",
    "    pre-processes a doc\n",
    "      * Converts the tweet into lower case,\n",
    "      * removes the URLs,\n",
    "      * removes the punctuations\n",
    "      * tokenizes the tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # gettign rid of non ascii codes\n",
    "    # doc = doc.decode('ascii', 'ignore')\n",
    "    \n",
    "    # repalcing URLs\n",
    "    url_pattern = \"http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+\"\n",
    "    doc = re.sub(url_pattern, 'url', doc) \n",
    "\n",
    "    punctuation = r\"\\(|\\)|#|\\'|\\\"|-|:|\\\\|\\/|!|_|,|=|;|>|<|\\.\"\n",
    "    doc = re.sub(punctuation, ' ', doc)\n",
    "    \n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BjnGhMLKJH3"
   },
   "outputs": [],
   "source": [
    "pre_process(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UDAgG7NKJIB"
   },
   "outputs": [],
   "source": [
    "#Use the function provided to pre-process one of the tweets.\n",
    "def termdoc(docs):\n",
    "    \"\"\"\n",
    "    returns the term-by-document matrix and the vocabulary of the passed corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()   \n",
    "    termdoc_sparse = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # pre-process the doc\n",
    "        doc_tokens = pre_process(doc)\n",
    "        # computes the frequencies for doc\n",
    "        doc_sparse = Counter(doc_tokens)    \n",
    "\n",
    "        termdoc_sparse.append(doc_sparse)\n",
    "\n",
    "        # update the vocab\n",
    "        vocab.update(doc_sparse.keys())  \n",
    "\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    n_vocab = len(vocab)\n",
    "    termdoc_dense = np.zeros((n_docs, n_vocab), dtype=int)\n",
    "\n",
    "    for j, doc_sparse in enumerate(termdoc_sparse):\n",
    "        for term, freq in doc_sparse.items():\n",
    "            termdoc_dense[j, vocab.index(term)] = freq\n",
    "            \n",
    "    return termdoc_dense, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPiPTWqfKJIV"
   },
   "outputs": [],
   "source": [
    "tweets_termdoc, tweets_vocab = termdoc(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CuRf7qSKJIv"
   },
   "source": [
    "Lets look at the vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "cVwvtu8HKJIy"
   },
   "source": [
    "Lets look at one of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8BtoGfnKJI0"
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "print (tweets[j])\n",
    "print (tweets_termdoc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1iQRuclKJJB"
   },
   "outputs": [],
   "source": [
    "tweets_vocab.index('beyond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5hi3TQGKJJH"
   },
   "outputs": [],
   "source": [
    "tweets_termdoc[j][127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCqATmO-KJJP"
   },
   "source": [
    "So baiscally, now each tweet is represented by a vector of size `len(tweets_vocab)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3kfybLZJKJJg"
   },
   "source": [
    "# 4.k-Nearest Neighbours Classification \n",
    "\n",
    "kNN is a non-parametric classification technique which is extensively used in practice. Its input consists of the `k` closest training examples and the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "**Please note that kNN is different from K-means.** K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster be close to each other. It is unsupervised because the points have no external classification. kNN is a classification algorithm that in order to determine the classification of a point, combines the class of the k nearest points. It is supervised because you are trying to classify a point based on the known label of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNJrn3xoKJJh"
   },
   "source": [
    "## 4.1.kNN in Python\n",
    "\n",
    "To be able to illustrate how we perform kNN classification in Python, we need some data first. Therefore we synthesize some data from 3 classes. We assume the data in each class comes from a multivariate random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REmnvBGZKJJj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5T6dPkAjKJJq"
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "n_per_class = 50\n",
    "colors = ['green', 'blue', 'magenta']\n",
    "\n",
    "mean1 = [-5, 10]\n",
    "cov1 = [[1.5, 0], [0, 1.5]]\n",
    "mean2 = [0, 7]\n",
    "cov2 = [[1.5, 0], [0, 3]]\n",
    "mean3 = [-6, 6]\n",
    "cov3 = [[2, 0], [0, 1.5]]\n",
    "\n",
    "means = [mean1, mean2, mean3]\n",
    "covs = [cov1, cov2, cov3]\n",
    "\n",
    "x11, x12 = np.random.multivariate_normal(means[0], covs[0], n_per_class).T\n",
    "x21, x22 = np.random.multivariate_normal(means[1], covs[1], n_per_class).T\n",
    "x31, x32 = np.random.multivariate_normal(means[2], covs[2], n_per_class).T\n",
    "\n",
    "scale = 75\n",
    "alpha = 0.6\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(7, 7), dpi=100)\n",
    "ax.scatter(x11, x12, alpha=alpha, color=colors[0], s=scale)\n",
    "ax.scatter(x21, x22, alpha=alpha, color=colors[1], s=scale)\n",
    "ax.scatter(x31, x32, alpha=alpha, color=colors[2], s=scale)\n",
    "\n",
    "ax.set_title(\"synthesized data for 3 classes\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_v1rt2g2KJJv"
   },
   "source": [
    "Then we have to instantiate a kNN classifier from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDNbbzchKJJz"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "weights='uniform'\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k,weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ae6l5eKhKJJ3"
   },
   "source": [
    "We need to pass one array as training features and on array as training labels to the `knn` object. Therefore we have to put all the attributes together (also class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8rJOfblKJJ4"
   },
   "outputs": [],
   "source": [
    "x1 = np.r_[x11, x21, x31]\n",
    "x2 = np.r_[x12, x22, x32]\n",
    "X_train = np.c_[x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZTNstLRKJJ8"
   },
   "outputs": [],
   "source": [
    "Y_train = np.r_[0*np.ones(n_per_class), 1*np.ones(n_per_class), 2*np.ones(n_per_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKT_KxOEKJKB"
   },
   "source": [
    "Now we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yqzUM2KKJKC"
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnZXWn8iKJKN"
   },
   "source": [
    "Now we will see how kNN classifies a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G30jt__IKJKP"
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyB9mO2tKJKV"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9oXQZfC-KJKi"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "X_test = [-7, 10]\n",
    "Y_pred = knn.predict([X_test])\n",
    "ax.scatter(X_test[0], X_test[1], marker=\"x\", s=scale, lw=2, c='k')\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTvKK8eqKJKo"
   },
   "source": [
    "## 4.2.Decision Boundry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwcSkX4rKJKp"
   },
   "source": [
    "kNN effectively partitions the feature space into different sets and assigns the same class label to points belonging to the same partition. This partitioning changes as we change k. We illustrate this below. As you see bigger values of k, partition the space more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNukvWvVKJKr"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pe0r7eHoKJKz"
   },
   "outputs": [],
   "source": [
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.05\n",
    "\n",
    "# Create colour maps\n",
    "cmap_light = ListedColormap(['#AAFFAA', '#AAAAFF', '#FFAAAA'])\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])\n",
    "\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgyCevpdKJK9"
   },
   "source": [
    "Now we will investigate the effect of `'k'` on decision boundaries. Lets train a classifier with `k=1` which means we only use the label of the closest point to predict the label of a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXNY43hOKJK_"
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNXSsdccKJLK"
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8V8mGkZXKJLO"
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKe69jl6KJLT"
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqMLDl5qKJLZ"
   },
   "source": [
    "#### 4.2.1.Prediction\n",
    "\n",
    "Play with the `X_test` and `k` to see how the classifier behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0aLX5LxEKJLc"
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "X_test = [-4, 8]\n",
    "Y_pred = knn.predict([X_test])\n",
    "ax.scatter(X_test[0], X_test[1], alpha=0.95, color='r', s=3*scale)\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rmq5u9UWKJLi"
   },
   "source": [
    "Now instead of predicting the class label for one point, we use our model to predict the labels of multiple points.\n",
    "\n",
    "First we generate some test data from the first class. This way we know the true class labels. Then we can use the `kNN` classifier to predict labels for the test data and get the predicted class labels. A measure of  accuracy for the classifier can be defined by comparing the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEbSuYkVKJLk"
   },
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "X1_test, X2_test = np.random.multivariate_normal(mean1, cov1, n_test).T\n",
    "Y_true = 0 * np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJeGtN-cKJLq"
   },
   "outputs": [],
   "source": [
    "X_test = np.c_[X1_test, X2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGNqTK_vKJLu"
   },
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4JJUJR0DKJLy"
   },
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9h3KZTQVKJL6"
   },
   "source": [
    "How many times the classifier predicts the labels correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdJj0U3SKJMC"
   },
   "outputs": [],
   "source": [
    "Y_pred == Y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rC2Ugu9TKJMK"
   },
   "source": [
    " Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vD7NeWdCKJMP"
   },
   "outputs": [],
   "source": [
    "(sum(Y_pred == Y_true) + 0.0) / n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIPQ8GZvKJMX"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQprhDczKJMc"
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IH5HjHf7KJMk"
   },
   "outputs": [],
   "source": [
    "digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7fdhxflKJMs"
   },
   "outputs": [],
   "source": [
    "digits['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yi4-xeNKJMv"
   },
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "Y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbwAGbYWKJMz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzoca4J0KJM3"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRVjBi8eKJM8"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OtDptITKJM_"
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxZo367zKJNC"
   },
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zr9exSbhKJNG"
   },
   "outputs": [],
   "source": [
    "Y_pred == Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nt5CL-0jKJNK"
   },
   "outputs": [],
   "source": [
    "(sum(Y_pred == Y_test) + 0.0) / len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJtHxxkGKJNQ"
   },
   "outputs": [],
   "source": [
    "mask = Y_pred != Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDgN0rERKJNU"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_test[mask][i, :].reshape(8, 8))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"{}\".format(Y_pred[mask][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-LPC1leKJNY"
   },
   "source": [
    "# 5.Naive Bayes Classifier\n",
    "\n",
    "\n",
    "Naive Bayes is one of the most practical classification machine learning algorithms. \n",
    "\n",
    "* fast\n",
    "* good performance\n",
    "* simple yet very effective\n",
    "* robust to irrelative features\n",
    "\n",
    "So why is it called naive?\n",
    "\n",
    "Because it does not consider the dependency between features and assume all features are independent of each other which is not the case in reality. This is a naive assumption, hence the name.\n",
    "\n",
    "The accuracy is very good although this naive assumption. A famous example of NB usage is spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WEcz1XH4KJNe"
   },
   "source": [
    "## 5.1.NBC by Example\n",
    "\n",
    "We assume we have collected the below data for the past 5 days. Based on this data, can we predict if our subject will play in a setting like:\n",
    "\n",
    "    outlook  = overcast\n",
    "    temp     = hot\n",
    "    humidity = normal\n",
    "    windy    = no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWW6fvBzKJNf"
   },
   "source": [
    "<!-- <img src=\"nb_data.png\" width=\"800\"> -->\n",
    "<img src=\"https://github.com/tuliplab/mds/raw/master/Jupyter/image/nb_data.png\" width=\"800\">\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAQXtJQ8KJNg"
   },
   "source": [
    "First we have to find a representation for our data. We can construct a dictionary to convert stings into numbers and then save them in a dataframe. \n",
    "\n",
    "    outlook: sunny=0, overcast=1, rainy=2\n",
    "    temp: hot=0, mild=1, cool=2\n",
    "    humidity: normal=0, high=1\n",
    "    wind: no=0, yes=1\n",
    "    play: np=0, yes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldO6IdOWKJNh"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FV_2yevmKJNl"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'outlook': [0, 1, 2, 0, 1],\n",
    "    'temp'   : [0, 1, 2, 1, 0],\n",
    "    'humid'  : [0, 0, 1, 0, 1],\n",
    "    'wind'   : [0, 0, 1, 1, 0],\n",
    "    'play'   : [1, 1, 0, 0, 0,]    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlsXoeESKJNo"
   },
   "source": [
    "Now we use Bayes rule to construct a Naive Bayes classifier. We can write:\n",
    "\n",
    "$$Pr\\left(p|o,t,h,w\\right)\\propto Pr\\left(p\\right)Pr(o|p)Pr(t|p)Pr(h|p)Pr(w|p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtZqI5K6KJNp"
   },
   "source": [
    "To calculate $Pr(p)$ we use marginal probablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkEYImO1KJNq"
   },
   "outputs": [],
   "source": [
    "def marginal_prob(df, col):\n",
    "    ll = [(ss, (df[col] == ss).sum()) for ss in set(df[col])]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgObf-C4KJNv"
   },
   "source": [
    "To calculate probability of a feature given the class (play) we use conditinoal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqRQyPSQKJNx"
   },
   "outputs": [],
   "source": [
    "def conditional_prob(df, f, c, val):\n",
    "    df2 = df[df[c] == val][f]\n",
    "    ll = [[ss, (df2 == ss).sum()] for ss in set(df2)]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucnAv-qJKJN1"
   },
   "source": [
    "Now we can use Bayes rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QimSTPITKJN2"
   },
   "outputs": [],
   "source": [
    "o = 1\n",
    "t = 0\n",
    "h = 0\n",
    "w = 0\n",
    "\n",
    "c = 0\n",
    "p0 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "c = 1\n",
    "p1 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "# normalizing\n",
    "p_sum = p0 + p1\n",
    "p0 /= p_sum\n",
    "p1 /= p_sum\n",
    "\n",
    "print (\"probability of not playing: {}\".format(p0))\n",
    "print (\"probability of playing    : {}\".format(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnqpTnsWKJN6"
   },
   "source": [
    "## 5.2.NBC Exercise\n",
    "Suppose we have documents below as our training set. \n",
    "\n",
    "    d1: Chinese Beijing Chinese , class = C\n",
    "    d2: Chinese Chinese Shanghai, class = C\n",
    "    d3: Chinese Macao           , class = C\n",
    "    d4: Tokyo Japan Chinese     , class = J\n",
    "Train a NB classifier and predict if `d5` belongs to class C or J.\n",
    "\n",
    "    d5: Chinese Chinese Chinese Tokyo Japan, class = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvrfKPreKJN7"
   },
   "outputs": [],
   "source": [
    "#Try to input Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svbA294BKJOB"
   },
   "source": [
    "# 6.Random Forest\n",
    "\n",
    "Random forests are an example of an *ensemble learner* built on decision trees.\n",
    "For this reason we'll start by discussing decision trees themselves.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWIU7ApoKJOC"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlR4jIoqKJOp"
   },
   "source": [
    "The binary splitting makes this extremely efficient.\n",
    "As always, though, the trick is to *ask the right questions*.\n",
    "This is where the algorithmic process comes in: in training a decision tree classifier, the algorithm looks at the features and decides which questions (or \"splits\") contain the most information.\n",
    "\n",
    "## 6.1.Creating a Decision Tree\n",
    "\n",
    "Here's an example of a decision tree classifier in scikit-learn. We'll start by defining some two-dimensional labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nyxhhKWZKJOy"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=10, n_informative=5,\n",
    "                  random_state=0, shuffle=False)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VcOy5_spKJPW"
   },
   "source": [
    "## 6.2.Random Forest Regressor\n",
    ".\n",
    "Random forests can be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is ``sklearn.ensemble.RandomForestRegressor``.\n",
    "\n",
    "Let's quickly demonstrate how this can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW1-USRnKJPX"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "x = 10 * np.random.rand(100)\n",
    "\n",
    "def model(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt='o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rifVgy64KJPb"
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = RandomForestRegressor(100).fit(x[:, None], y).predict(xfit[:, None])\n",
    "ytrue = model(xfit, 0)\n",
    "\n",
    "plt.errorbar(x, y, 0.3, fmt='o')\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSRhkhJMKJPh"
   },
   "source": [
    "As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us even specifying a multi-period model!\n",
    "\n",
    "Tradeoff between simplicity and thinking about what your data is.\n",
    "\n",
    "Feature engineering is important, need to know your domain: Fourier transform frequency distribution.\n",
    "\n",
    "## 6.3.Random Forest Limitations\n",
    "\n",
    "The following data scenarios are not well suited for random forests:\n",
    "* y: lots of 0, few 1\n",
    "* Structured data like images where a neural network might be better\n",
    "* Small data size which might lead to overfitting\n",
    "* High dimensional data where a linear model might work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRswI8SCKJPj"
   },
   "source": [
    "# 7.Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIA1XE7rKJPm"
   },
   "source": [
    "## 7.1.Population vs Sample \n",
    "\n",
    "The main difference between population and sample comes down to how observations are assigned to dataset. A **population** includes all of the elements from the dataset. A **sample** consists of one or more observations from the population. In other words **population** is the entire collection of the desired measurable characteristic that we would have, if we could collect it. \n",
    "\n",
    "For example if suppose we want to find the average height of 2nd grade students in Australia. The population would be all the students who are studying in 2nd grade in Australia. But most probably we can not measure the height of all Australian 2nd grade students. It is not feasible. So what do we do? **We sample!**. We collect the height of some of Australian 2nd graders and based on that, we **estimate** the average height of the population (all of Australian 2nd grade students). The sample could be 2nd grade students of one class in one school, or multiple classes in multiple schools in one state, or  multiple schools in multiple states, or etc.\n",
    "\n",
    "A measurable statistic of a population (such as mean) is called a **parameter**. But a measurable characteristic of a sample is called **statistic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HAPcV20o1jz"
   },
   "source": [
    "## 7.2.Confidence Interval\n",
    "As stated in previous section, population parameter is unknown. Confidence interval is a type of interval estimate of a population parameter calculated from sample statistics. It is an interval estimate combined with a probability.\n",
    "\n",
    "For the aforementioned example, it means that without collecting the height of all Aussie 2nd graders, we can estimate the average height by collecting a sample and using the below formula:\n",
    "\n",
    "\n",
    "$$Confidence\\, Interval=\\bar{X}\\pm z\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "$s$ is the sample standard deviation, $n$ is the sample size, and $z$ is often read from a table.\n",
    "\n",
    "    confidence level (%)     z\n",
    "         \n",
    "            70              1.04 \n",
    "            75              1.15\n",
    "            80              1.28 \n",
    "            85              1.44 \n",
    "            90              1.645\n",
    "            92              1.75\n",
    "            95              1.96\n",
    "            96              2.05\n",
    "            98              2.33\n",
    "            99              2.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19exHjtSKJPq"
   },
   "source": [
    "Now lets use an example to clarify this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwCTOcmyKJPr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhC7x3wPKJPw"
   },
   "source": [
    "First we create a population. Please note that we do not use the population in our computations. We use samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUM-NUAgKJPz"
   },
   "outputs": [],
   "source": [
    "mu, sigma = 10, 2\n",
    "n_population = 10000\n",
    "population = np.random.normal(mu, sigma, n_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9DcOe51KJP4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(population, bins=25)\n",
    "ax.set_title(r\"population histogram, $\\mu={}$, $\\sigma={}$ \".format(mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bE1Tnzm-KJP7"
   },
   "source": [
    "Now we sample multiple times from this population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gu6NxqLKJP8"
   },
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "sample_size = 500\n",
    "samples = np.zeros([n_trials, sample_size])\n",
    "sample_std = np.zeros(n_trials)\n",
    "sample_mean = np.zeros(n_trials)\n",
    "\n",
    "for i in range(n_trials):\n",
    "    samples[i] = np.random.choice(population, size=sample_size, replace=False)\n",
    "    sample_mean[i] = samples[i].mean()\n",
    "    sample_std[i] = samples[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70syMH9QKJP-"
   },
   "outputs": [],
   "source": [
    "i = 200\n",
    "se = sample_std[i] / np.sqrt(sample_size)\n",
    "dist = ss.distributions.norm(sample_mean[i], se)\n",
    "z = 1.96\n",
    "\n",
    "x = np.linspace(9.5, 10.5, 100)\n",
    "y = dist.pdf(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "    \n",
    "ax.hist(sample_mean, density=True, bins=20, label='sample means')\n",
    "ax.plot(x, y, label=\"$N(\\mu_{i},se)$\")\n",
    "ax.vlines(sample_mean[i] + z*se, 0, 5, label='$\\mu_i \\pm z\\sigma_i$')\n",
    "ax.vlines(sample_mean[i] - z*se, 0, 5)\n",
    "\n",
    "ax.set_title(\"distribution of sample statistics\\ni={}\".format(i))\n",
    "ax.legend()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hSRhkhJMKJPh"
   ],
   "name": "SIT742P08B-Supervised.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
